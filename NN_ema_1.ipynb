{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models with Gaussian noise regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "\n",
    "#tensorflow-related\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import feature_column\n",
    "\n",
    "#basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn-related\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#custom\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "#tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Proprietario/opt/anaconda3/envs/dslab1/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (50,78,80,91,92,93,94,95,105,106,107,108,109,119,120,121,123,133,134,135,137,276,291,292,295,296,297,300,301,302,305,306,307) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# import data: MRI/EEG/...\n",
    "\n",
    "dataframe = utils.create_dataset_mri(select_disease = None, select_category = None, SCORE = 'Age', thickness= True, volume=True, subcortical=True, DTI = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect which columns have nan samples and find which samples are those\n",
    "\n",
    "def find_columns_with_nan_samples(dataframe): \n",
    "    \n",
    "    # dataframe is a pandas DataFrame\n",
    "    \n",
    "    columns_nans = {}\n",
    "    for column in dataframe.columns: \n",
    "        nan_col_vector = dataframe[column].isna()\n",
    "        samples_with_nan = []\n",
    "        if nan_col_vector.values.any(): \n",
    "            for index, value in enumerate(nan_col_vector):\n",
    "                if value == True: \n",
    "                    samples_with_nan.append(index)\n",
    "            columns_nans[column] =  samples_with_nan\n",
    "    return columns_nans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns with object as dtype\n",
    "\n",
    "def find_columns_with_dtype_object(dataframe):\n",
    "    \n",
    "    columns_object_type = []\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].dtypes == 'object': \n",
    "            columns_object_type.append(column)\n",
    "    return columns_object_type\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DX_01_Cat': [1110], 'DX_01_Sub': [0, 1, 4, 8, 10, 11, 14, 17, 18, 19, 20, 21, 27, 28, 29, 30, 33, 37, 40, 41, 42, 44, 45, 49, 50, 52, 53, 55, 58, 59, 60, 64, 65, 68, 69, 71, 72, 74, 76, 77, 79, 80, 81, 82, 83, 87, 88, 93, 94, 95, 98, 99, 100, 101, 105, 107, 110, 112, 113, 115, 116, 117, 118, 119, 120, 121, 123, 124, 129, 131, 132, 133, 136, 137, 144, 147, 152, 160, 161, 162, 165, 166, 169, 170, 174, 180, 185, 187, 189, 191, 192, 194, 196, 197, 199, 200, 203, 204, 211, 212, 213, 216, 220, 221, 222, 225, 226, 232, 236, 237, 239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 263, 265, 266, 267, 268, 270, 271, 277, 279, 280, 281, 282, 283, 287, 288, 289, 290, 292, 295, 296, 297, 300, 301, 305, 307, 310, 312, 313, 314, 316, 317, 318, 320, 321, 323, 327, 328, 329, 330, 333, 339, 340, 351, 352, 354, 355, 356, 357, 358, 362, 363, 365, 366, 368, 376, 377, 381, 382, 385, 386, 388, 393, 397, 403, 404, 405, 409, 410, 411, 413, 415, 419, 425, 426, 431, 432, 434, 436, 437, 444, 448, 451, 458, 463, 466, 468, 470, 473, 478, 479, 480, 481, 483, 485, 487, 489, 490, 491, 492, 495, 497, 500, 501, 504, 505, 506, 507, 510, 511, 512, 515, 517, 520, 521, 522, 523, 525, 527, 530, 533, 534, 535, 541, 542, 544, 546, 549, 550, 551, 556, 558, 559, 563, 564, 568, 576, 580, 582, 584, 585, 586, 590, 591, 598, 603, 605, 606, 607, 608, 609, 611, 612, 614, 615, 617, 619, 627, 628, 634, 635, 636, 638, 639, 642, 643, 647, 648, 649, 651, 659, 664, 665, 667, 668, 672, 677, 682, 683, 685, 686, 688, 690, 691, 694, 696, 702, 703, 705, 706, 708, 709, 721, 730, 740, 743, 747, 748, 749, 750, 754, 756, 757, 758, 761, 764, 770, 772, 774, 776, 780, 783, 784, 787, 796, 797, 798, 800, 801, 802, 806, 810, 811, 815, 816, 823, 829, 834, 837, 840, 843, 844, 845, 849, 852, 856, 858, 860, 862, 863, 872, 874, 875, 876, 877, 878, 880, 881, 882, 885, 887, 889, 890, 891, 895, 896, 903, 906, 907, 909, 910, 911, 912, 914, 915, 917, 922, 923, 927, 928, 930, 931, 935, 939, 947, 951, 962, 963, 964, 965, 972, 979, 984, 985, 997, 1005, 1008, 1019, 1020, 1023, 1025, 1027, 1031, 1033, 1035, 1036, 1037, 1039, 1041, 1042, 1044, 1045, 1046, 1047, 1050, 1053, 1054, 1057, 1058, 1061, 1062, 1064, 1065, 1066, 1067, 1072, 1075, 1077, 1078, 1079, 1081, 1082, 1086, 1087, 1088, 1091, 1095, 1099, 1107, 1110, 1112, 1114, 1117, 1120, 1122, 1124, 1127, 1128, 1129, 1134, 1135, 1136, 1137, 1140, 1141, 1142, 1143], 'DX_01': [4, 42, 58, 116, 136, 271, 320, 330, 351, 355, 393, 403, 405, 505, 611, 615, 705, 750, 823, 849, 964, 1110]}\n",
      "['ID', 'DX_01_Cat', 'DX_01_Sub', 'DX_01']\n"
     ]
    }
   ],
   "source": [
    "cols_with_nans = find_columns_with_nan_samples(dataframe)\n",
    "print(cols_with_nans)\n",
    "\n",
    "obj_cols = find_columns_with_dtype_object(dataframe)\n",
    "print(obj_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then one sees what columns with NaN and/or 'object' as type one wants to keep. In this case kept 'DX_01_Cat'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop samples that are nan in selected columns \n",
    "for sample in cols_with_nans['DX_01_Cat']:\n",
    "    dataframe = dataframe.drop(sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert object variables into categorical variables (if they don't have too much NaN)\n",
    "dataframe['DX_01_Cat'] = pd.Categorical(dataframe['DX_01_Cat'])\n",
    "dataframe['DX_01_Cat'] = dataframe['DX_01_Cat'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns with NaNs and/or 'object' as type + 'ID'\n",
    "id_column = dataframe.pop('ID')\n",
    "dataframe = dataframe.drop(['DX_01_Sub', 'DX_01'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                               float64\n",
       "DX_01_Cat                            int8\n",
       "lh_G.S_frontomargin_thickness     float64\n",
       "lh_G.S_occipital_inf_thickness    float64\n",
       "lh_G.S_paracentral_thickness      float64\n",
       "                                   ...   \n",
       "rh_frontalpole_volume               int64\n",
       "rh_temporalpole_volume              int64\n",
       "rh_transversetemporal_volume        int64\n",
       "rh_insula_volume                    int64\n",
       "GlobalCorticalThickness           float64\n",
       "Length: 371, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print dtype of the columns: useful for feature_columns\n",
    "dataframe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT TARGET VARIABLE\n",
    "target_variable = 'Age'     #header of the variable to use as label (target), i.e. value to be predicted\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "Gaussian_regularization = True\n",
    "std_dev = tf.Variable(1.0)     # std deviation of added Gaussian noise\n",
    "\n",
    "# PIPELINE PARAMETERS\n",
    "batch_size_train = 8\n",
    "batch_size_eval = 8\n",
    "n_epochs = 2\n",
    "loss ='mse'          # mean squared error\n",
    "metrics =['mae']    # mean absolute error\n",
    "test_set_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe_c = dataframe.copy()\n",
    "target = dataframe_c.pop(target_variable)\n",
    "\n",
    "#TRAIN/TEST SPLIT\n",
    "dataframe_train, dataframe_test, target_train, target_test = train_test_split(dataframe_c, target, test_size=test_set_size, shuffle=True)\n",
    "\n",
    "# DATASET API \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(dataframe_train), target_train)).batch(batch_size_train)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(dataframe_test), target_test)).batch(batch_size_eval)\n",
    "\n",
    "#FEATURE LAYER \n",
    "#columns are all numeric \n",
    "feature_columns = []\n",
    "\n",
    "for header in dataframe_c.columns:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "    \n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE MODEL\n",
    "model = tf.keras.Sequential()\n",
    "model.add(feature_layer)\n",
    "if Gaussian_regularization == True:\n",
    "    model.add(layers.GaussianNoise(std_dev))        # Gaussian Noise regularization layer\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation=\"linear\"))\n",
    "\n",
    "\n",
    "#CONFIGURE MODEL\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss=loss,      \n",
    "              metrics=metrics)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "WARNING:tensorflow:Layer sequential is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/2\n",
      "115/115 [==============================] - 13s 113ms/step - loss: 16.2932 - mae: 3.2412\n",
      "Epoch 2/2\n",
      "115/115 [==============================] - 3s 24ms/step - loss: 14.0322 - mae: 3.0977\n",
      "Losses: [16.234350142000025, 14.101581378795174]\n"
     ]
    }
   ],
   "source": [
    "#TRAIN MODEL\n",
    "print('# Fit model on training data')\n",
    "history = model.fit(train_dataset, epochs=n_epochs)\n",
    "\n",
    "print ('Losses:', history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "29/29 [==============================] - 5s 185ms/step - loss: 14.1008 - mae: 2.9098\n",
      "mse, mae: 14.100794611306027 2.909834\n"
     ]
    }
   ],
   "source": [
    "# TEST MODEL\n",
    "print('\\n# Evaluate on test data')\n",
    "mse, mae = model.evaluate(test_dataset)\n",
    "print('mse, mae:', mse, mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
