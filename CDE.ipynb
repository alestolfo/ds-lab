{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional density estimation with noise regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/camillca/anaconda3/envs/dslab/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import create_dataset_mri, cv_for_cde, create_dataset_eeg\n",
    "from cde.density_estimator import MixtureDensityNetwork\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.python.keras.activations import tanh\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# torch version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "target = 'Age'\n",
    "# use all MRI high-level features, no DTI\n",
    "data = create_dataset_mri(SCORE = target)\n",
    "# for the moment, remove the diagnosis colums\n",
    "data.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1146, 369)\n",
      "(1146,)\n"
     ]
    }
   ],
   "source": [
    "# labels and features\n",
    "y = data[target]\n",
    "X = data.drop([target, 'ID'], axis=1)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h = 0.9818524371920933\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 10s | loss: 1300.356\n",
      "mean log-loss train: 1.4562\n",
      "Test MSE: 4.840087141773545\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters\n",
    "ndim_x=X_train.shape[1]\n",
    "ndim_y=y_train.shape[1]\n",
    "# We try the \"faster decay rate for non-gaussian data\" proposed in the paper: h = n^(-1/(d+1))\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]+y_train.shape[1]\n",
    "h = n**(-1/(d+1))\n",
    "print('h = {}'.format(h))\n",
    "\n",
    "# Define the model\n",
    "model = MixtureDensityNetwork('MDN_mri', ndim_x, ndim_y, n_centers=10, hidden_sizes=(16, 16), hidden_nonlinearity=tf.nn.tanh,\n",
    "               n_training_epochs=1000, x_noise_std=h, y_noise_std=h, adaptive_noise_fn=None, entropy_reg_coef=0.0,\n",
    "               weight_decay=0.0, weight_normalization=True, data_normalization=True, dropout=0.0, l2_reg=0.0, l1_reg=0.0,\n",
    "               random_seed=42)\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = model.mean_(X_test)\n",
    "y_pred = y_pred.reshape((-1,1))\n",
    "y_pred.shape\n",
    "print('Test MSE: {}'.format(mean_squared_error(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 11s | loss: 1065.688\n",
      "mean log-loss train: 1.4926\n",
      "MSE: 4.645851777119789\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 11s | loss: 1024.148\n",
      "mean log-loss train: 1.4344\n",
      "MSE: 4.778558200695333\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 12s | loss: 1045.872\n",
      "mean log-loss train: 1.4648\n",
      "MSE: 5.456636122440835\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 12s | loss: 1014.773\n",
      "mean log-loss train: 1.4193\n",
      "MSE: 6.753902714941601\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 12s | loss: 1052.664\n",
      "mean log-loss train: 1.4723\n",
      "MSE: 4.895140938532837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.30601795074608"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV results\n",
    "res = cv_for_cde(X_train, y_train.flatten(), 'mri', h, n_splits=5)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural MRI + DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "target = 'Age'\n",
    "# use all MRI high-level features, plus DTI\n",
    "data = create_dataset_mri(SCORE = target, DTI = True)\n",
    "# for the moment, remove the diagnosis colums\n",
    "data.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(838, 390)\n",
      "(838,)\n"
     ]
    }
   ],
   "source": [
    "# labels and features\n",
    "y = data[target]\n",
    "X = data.drop([target, 'ID'], axis=1)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.reshape((-1,1))\n",
    "# Impute missing DTI values\n",
    "imp = SimpleImputer(strategy = 'median')\n",
    "X = imp.fit_transform(X)\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 12s | loss: 950.702\n",
      "mean log-loss train: 1.4559\n",
      "Test MSE: 4.517503224792356\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters\n",
    "ndim_x=X_train.shape[1]\n",
    "ndim_y=y_train.shape[1]\n",
    "# We try the \"faster decay rate for non-gaussian data\" proposed in the paper: h = n^(-1/(d+1))\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]+y.shape[1]\n",
    "h = n**(-1/(d+1))\n",
    "# Define the model\n",
    "model = MixtureDensityNetwork('MDN_dti', ndim_x, ndim_y, n_centers=10, hidden_sizes=(16, 16), hidden_nonlinearity=tf.nn.tanh,\n",
    "               n_training_epochs=1000, x_noise_std=h, y_noise_std=h, adaptive_noise_fn=None, entropy_reg_coef=0.0,\n",
    "               weight_decay=0.0, weight_normalization=True, data_normalization=True, dropout=0.0, l2_reg=0.0, l1_reg=0.0,\n",
    "               random_seed=42)\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = model.mean_(X_test)\n",
    "y_pred = y_pred.reshape((-1,1))\n",
    "y_pred.shape\n",
    "print('Test MSE: {}'.format(mean_squared_error(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 13s | loss: 788.403\n",
      "mean log-loss train: 1.5104\n",
      "MSE: 5.876351321458095\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 14s | loss: 730.318\n",
      "mean log-loss train: 1.3991\n",
      "MSE: 5.383017929219776\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 14s | loss: 769.633\n",
      "mean log-loss train: 1.4744\n",
      "MSE: 4.347566019282965\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 14s | loss: 757.914\n",
      "mean log-loss train: 1.4492\n",
      "MSE: 7.485911999841195\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 16s | loss: 748.438\n",
      "mean log-loss train: 1.4310\n",
      "MSE: 4.518440574361241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.5222575688326545"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV results\n",
    "res = cv_for_cde(X_train, y_train.flatten(), 'dti', h, n_splits=5)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "target = 'Age'\n",
    "# eeg cluster features\n",
    "data = create_dataset_eeg(SCORE = target, clusters = True)\n",
    "# for the moment, remove the diagnosis colums\n",
    "data.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)\n",
    "# Features and labels\n",
    "y = data[target]\n",
    "X = data.drop([target, 'id'], axis=1)\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "y = y.reshape((-1,1))\n",
    "# Impute missing EEG values\n",
    "imp = SimpleImputer(strategy = 'median')\n",
    "X = imp.fit_transform(X)\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h = 0.9765226193230893\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 17s | loss: 1542.786\n",
      "mean log-loss train: 1.5155\n",
      "Test MSE: 5.594303000894464\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters\n",
    "ndim_x=X_train.shape[1]\n",
    "ndim_y=y_train.shape[1]\n",
    "# We try the \"faster decay rate for non-gaussian data\" proposed in the paper: h = n^(-1/(d+1))\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]+y.shape[1]\n",
    "h = n**(-1/(d+1))\n",
    "print('h = {}'.format(h))\n",
    "# Define the model\n",
    "model = MixtureDensityNetwork('MDN_eeg', ndim_x, ndim_y, n_centers=10, hidden_sizes=(16, 16), hidden_nonlinearity=tf.nn.tanh,\n",
    "               n_training_epochs=1000, x_noise_std=h, y_noise_std=h, adaptive_noise_fn=None, entropy_reg_coef=0.0,\n",
    "               weight_decay=0.0, weight_normalization=True, data_normalization=True, dropout=0.0, l2_reg=0.0, l1_reg=0.0,\n",
    "               random_seed=42)\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = model.mean_(X_test)\n",
    "y_pred = y_pred.reshape((-1,1))\n",
    "y_pred.shape\n",
    "print('Test MSE: {}'.format(mean_squared_error(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 17s | loss: 1261.812\n",
      "mean log-loss train: 1.5501\n",
      "MSE: 5.361740153461321\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 18s | loss: 1251.756\n",
      "mean log-loss train: 1.5378\n",
      "MSE: 6.201627949652423\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 19s | loss: 1206.154\n",
      "mean log-loss train: 1.4818\n",
      "MSE: 6.097960528900607\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 20s | loss: 1237.301\n",
      "mean log-loss train: 1.5182\n",
      "MSE: 6.487517571261879\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 22s | loss: 1266.509\n",
      "mean log-loss train: 1.5540\n",
      "MSE: 5.262103052869523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.8821898512291515"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV results\n",
    "res = cv_for_cde(X_train, y_train.flatten(), 'eeg', h, n_splits=5)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessary to scale the input data (the model already does normalization).\n",
    "Note that with DTI we obtain results similar to pure MRI. EEG data gives the worst results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab",
   "language": "python",
   "name": "dslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
