{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional density estimation with noise regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/camillca/anaconda3/envs/dslab/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import create_dataset_mri, cv_for_cde, create_dataset_eeg\n",
    "from cde.density_estimator import MixtureDensityNetwork\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.python.keras.activations import tanh\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# torch version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "target = 'Age'\n",
    "# use all MRI high-level features, no DTI\n",
    "data = create_dataset_mri(SCORE = target)\n",
    "# for the moment, remove the diagnosis colums\n",
    "data.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1146, 369)\n",
      "(1146,)\n"
     ]
    }
   ],
   "source": [
    "# labels and features\n",
    "y = data[target]\n",
    "X = data.drop([target, 'ID'], axis=1)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h = 0.9818524371920933\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 11s | loss: 1300.356\n",
      "mean log-loss train: 1.4562\n",
      "Test MSE: 4.840087141773545\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters\n",
    "ndim_x=X_train.shape[1]\n",
    "ndim_y=y_train.shape[1]\n",
    "# We try the \"faster decay rate for non-gaussian data\" proposed in the paper: h = n^(-1/(d+1))\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]+y_train.shape[1]\n",
    "h = n**(-1/(d+1))\n",
    "print('h = {}'.format(h))\n",
    "\n",
    "# Define the model\n",
    "model = MixtureDensityNetwork('MDN_mri', ndim_x, ndim_y, n_centers=10, hidden_sizes=(16, 16), hidden_nonlinearity=tf.nn.tanh,\n",
    "               n_training_epochs=1000, x_noise_std=h, y_noise_std=h, adaptive_noise_fn=None, entropy_reg_coef=0.0,\n",
    "               weight_decay=0.0, weight_normalization=True, data_normalization=True, dropout=0.0, l2_reg=0.0, l1_reg=0.0,\n",
    "               random_seed=42)\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = model.mean_(X_test)\n",
    "y_pred = y_pred.reshape((-1,1))\n",
    "y_pred.shape\n",
    "print('Test MSE: {}'.format(mean_squared_error(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 10s | loss: -469.010\n",
      "mean log-loss train: -0.6569\n",
      "MSE: 5.056738633647421\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 20s | loss: -484.294\n",
      "mean log-loss train: -0.6783\n",
      "MSE: 4.961049731611744\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 10s | loss: -519.910\n",
      "mean log-loss train: -0.7282\n",
      "MSE: 6.214937640528559\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 10s | loss: -495.980\n",
      "mean log-loss train: -0.6937\n",
      "MSE: 5.245402047576165\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 11s | loss: -526.526\n",
      "mean log-loss train: -0.7364\n",
      "MSE: 6.50257150902764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.596139912478305"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV results\n",
    "res = cv_for_cde(data=X_train, labels=y_train.flatten(), name = 'mri', n_splits=5)\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural MRI + DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "target = 'Age'\n",
    "# use all MRI high-level features, plus DTI\n",
    "data = create_dataset_mri(SCORE = target, DTI = True)\n",
    "# for the moment, remove the diagnosis colums\n",
    "data.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(838, 390)\n",
      "(838,)\n"
     ]
    }
   ],
   "source": [
    "# labels and features\n",
    "y = data[target]\n",
    "X = data.drop([target, 'ID'], axis=1)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.reshape((-1,1))\n",
    "# Impute missing DTI values\n",
    "imp = SimpleImputer(strategy = 'median')\n",
    "X = imp.fit_transform(X)\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 11s | loss: 950.702\n",
      "mean log-loss train: 1.4559\n",
      "Test MSE: 4.517503224792356\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters\n",
    "ndim_x=X_train.shape[1]\n",
    "ndim_y=y_train.shape[1]\n",
    "# We try the \"faster decay rate for non-gaussian data\" proposed in the paper: h = n^(-1/(d+1))\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]+y.shape[1]\n",
    "h = n**(-1/(d+1))\n",
    "# Define the model\n",
    "model = MixtureDensityNetwork('MDN_dti', ndim_x, ndim_y, n_centers=10, hidden_sizes=(16, 16), hidden_nonlinearity=tf.nn.tanh,\n",
    "               n_training_epochs=1000, x_noise_std=h, y_noise_std=h, adaptive_noise_fn=None, entropy_reg_coef=0.0,\n",
    "               weight_decay=0.0, weight_normalization=True, data_normalization=True, dropout=0.0, l2_reg=0.0, l1_reg=0.0,\n",
    "               random_seed=42)\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = model.mean_(X_test)\n",
    "y_pred = y_pred.reshape((-1,1))\n",
    "y_pred.shape\n",
    "print('Test MSE: {}'.format(mean_squared_error(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 11s | loss: -383.187\n",
      "mean log-loss train: -0.7341\n",
      "MSE: 4.859856948265746\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 12s | loss: -387.726\n",
      "mean log-loss train: -0.7428\n",
      "MSE: 6.701996293895531\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 13s | loss: -415.854\n",
      "mean log-loss train: -0.7967\n",
      "MSE: 6.577864446742694\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 14s | loss: -384.318\n",
      "mean log-loss train: -0.7348\n",
      "MSE: 6.590415030457063\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 15s | loss: -370.332\n",
      "mean log-loss train: -0.7081\n",
      "MSE: 4.778028214741852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.901632186820577"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV results\n",
    "res = cv_for_cde(data=X_train, labels=y_train.flatten(), n_splits=5, name = 'dti')\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "target = 'Age'\n",
    "# eeg cluster features\n",
    "data = create_dataset_eeg(SCORE = target, clusters = True)\n",
    "# for the moment, remove the diagnosis colums\n",
    "data.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)\n",
    "# Features and labels\n",
    "y = data[target]\n",
    "X = data.drop([target, 'id'], axis=1)\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "y = y.reshape((-1,1))\n",
    "# Impute missing EEG values\n",
    "imp = SimpleImputer(strategy = 'median')\n",
    "X = imp.fit_transform(X)\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h = 0.9765226193230893\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 49s | loss: 1542.786\n",
      "mean log-loss train: 1.5155\n",
      "Test MSE: 5.594303000894464\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters\n",
    "ndim_x=X_train.shape[1]\n",
    "ndim_y=y_train.shape[1]\n",
    "# We try the \"faster decay rate for non-gaussian data\" proposed in the paper: h = n^(-1/(d+1))\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]+y.shape[1]\n",
    "h = n**(-1/(d+1))\n",
    "print('h = {}'.format(h))\n",
    "# Define the model\n",
    "model = MixtureDensityNetwork('MDN_eeg', ndim_x, ndim_y, n_centers=10, hidden_sizes=(16, 16), hidden_nonlinearity=tf.nn.tanh,\n",
    "               n_training_epochs=1000, x_noise_std=h, y_noise_std=h, adaptive_noise_fn=None, entropy_reg_coef=0.0,\n",
    "               weight_decay=0.0, weight_normalization=True, data_normalization=True, dropout=0.0, l2_reg=0.0, l1_reg=0.0,\n",
    "               random_seed=42)\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = model.mean_(X_test)\n",
    "y_pred = y_pred.reshape((-1,1))\n",
    "y_pred.shape\n",
    "print('Test MSE: {}'.format(mean_squared_error(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 67s | loss: -179.687\n",
      "mean log-loss train: -0.2207\n",
      "MSE: 6.189956021311957\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 58s | loss: -234.595\n",
      "mean log-loss train: -0.2882\n",
      "MSE: 7.306372974784494\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 59s | loss: -239.557\n",
      "mean log-loss train: -0.2943\n",
      "MSE: 6.0561494244488205\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 70s | loss: -199.253\n",
      "mean log-loss train: -0.2445\n",
      "MSE: 5.789622660751444\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 67s | loss: -248.613\n",
      "mean log-loss train: -0.3050\n",
      "MSE: 5.778252691272942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.2240707545139315"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CV results\n",
    "res = cv_for_cde(data=X_train, labels=y_train.flatten(), n_splits=5, name = 'eeg')\n",
    "np.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessary to scale the input data (the model already does normalization).\n",
    "Note that with DTI we obtain results similar to pure MRI. EEG data gives the worst results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dslab",
   "language": "python",
   "name": "dslab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
