{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import create_dataset_mri, create_dataset_eeg, create_dataset_eeg_old\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nans_based_on_column(dataframe, column): \n",
    "    nans_indices = []\n",
    "    for index, value in enumerate(dataframe.loc[:,column].isna()):\n",
    "        if value==True: \n",
    "            nans_indices.append(index)\n",
    "    print('dropped', len(nans_indices), 'samples')\n",
    "\n",
    "    return dataframe.drop(nans_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_nulls_based_on_column(dataframe, column): \n",
    "    nulls_indices = []\n",
    "    for index, value in enumerate(dataframe.loc[:,column].isnull()):\n",
    "        if value==True: \n",
    "            nulls_indices.append(index)\n",
    "    print('dropped', len(nulls_indices), 'samples')\n",
    "\n",
    "    return dataframe.drop(nulls_indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics_column(dataframe, column, show_histo=True):\n",
    "    \n",
    "    values = set(dataframe.loc[:,column].values)\n",
    "    values_dict = {}\n",
    "    for value in values: \n",
    "        values_dict[value] = len(dataframe.groupby(column).get_group(value).index)\n",
    "    if show_histo == True:\n",
    "        plt.bar(range(len(values_dict)), list(values_dict.values()), align='center', color='lightblue')\n",
    "        plt.xticks(range(len(values_dict)), range(1,len(values_dict)+1))\n",
    "        plt.show()\n",
    "    return values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get params for model to be inserted into sklearn pipeline\n",
    "def make_pipe_model_params(params): \n",
    "    new_params = {}\n",
    "    for key in params.keys(): \n",
    "        newkey = 'model_class__'+key\n",
    "        new_params[newkey] = params[key]\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get params for model to be inserted into sklearn pipeline (when using also pca)\n",
    "def make_pipe_model_params_pca(params): \n",
    "    new_params = {}\n",
    "    for key in params.keys():\n",
    "        if key != 'n_components':\n",
    "            newkey = 'model_class__'+key\n",
    "            new_params[newkey] = params[key]\n",
    "        else:\n",
    "            newkey = 'pca__'+key\n",
    "            new_params[newkey] = params[key]\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for cross-validation in One-Class Classification\n",
    "def cv_one_class_classification(model, data, labels, n_splits = 5):\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    '''\n",
    "    model: must be a sklearn object with .fit and .predict methods\n",
    "    data: the X matrix containing the features, can be a pd.DataFrame or a np object (array or matrix)\n",
    "    labels: y, can be a pd.DataFrame or a np array\n",
    "    n_splits: number of desired folds\n",
    "    => returns array of mean suqared error calculated on each fold\n",
    "    '''\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    scores = {\n",
    "        'balanced_accuracy' : [],\n",
    "        'f1_score': []\n",
    "    }\n",
    "    i = 1\n",
    "    for train, test in kf.split(data):\n",
    "        print(\"Split: {}\".format(i), end=\"\\r\")\n",
    "        X_train, X_test, y_train, y_test = data[train], data[test], labels[train], labels[test]\n",
    "        model.fit(X=X_train, y=y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        bal_accuracy = balanced_accuracy_score(y_true=y_test, y_pred=pred)\n",
    "        f1 = f1_score(y_true=y_test, y_pred=pred)\n",
    "        scores['balanced_accuracy'].append(bal_accuracy)\n",
    "        scores['f1_score'].append(f1)\n",
    "        i = i+1\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_class_classify_CV(X, y, models={'LinearSVC': 5}, CV_n_splits = 5, verbose=2, impute_strategy='mean'):\n",
    "    '''\n",
    "    implements CV using 'CV helper function for One-Class Classification'\n",
    "    input: dataset X, label y, dict of models to be used as 'model name' : number of combinations of parameters\n",
    "    output: 2 dictionaries \n",
    "    '''\n",
    "    # import libraries\n",
    "    import random\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    # results list to be returned\n",
    "    results= {}\n",
    "    bests= {}\n",
    "    \n",
    "    \n",
    "    # Ridge Classification ----------------------------------------------------------------------------------------------\n",
    "    if 'RidgeClassifier' in models.keys(): \n",
    "        # model class\n",
    "        from sklearn.linear_model import RidgeClassifier\n",
    "        model_class_name = 'RidgeClassifier'\n",
    "        model_class = RidgeClassifier()\n",
    "        #create list for model class results\n",
    "        model_class_results = []\n",
    "        # get number of combinations\n",
    "        n_param_combinations = models.get(model_class_name)\n",
    "        # parameter grid for the model class\n",
    "        param_grid ={\n",
    "            'alpha' : [0.5, 0.7, 1.0, 1.1, 1.3, 1.5, 2.0]\n",
    "            }\n",
    "        #create list of parameter combinations\n",
    "        p_grids = list(ParameterGrid(param_grid))\n",
    "        total_combinations = len(p_grids)\n",
    "        if n_param_combinations >= total_combinations: \n",
    "            print('trying all possible combiations of parameters for ',model_class_name)\n",
    "        else: \n",
    "            print('trying ', n_param_combinations,' possible combiations of parameters for ',model_class_name)\n",
    "        # loop over parameter combinations\n",
    "        i = 1\n",
    "        while (i <= n_param_combinations) and (i <= total_combinations):\n",
    "            try:\n",
    "                params = random.choice(p_grids)\n",
    "                print(params)\n",
    "                p_grids.remove(params)\n",
    "                pipe = Pipeline([('imputing',SimpleImputer(strategy=impute_strategy)),('scaling', StandardScaler()), ('model_class', model_class)])\n",
    "                pipe_params = make_pipe_model_params(params)\n",
    "                model = pipe.set_params(**pipe_params)\n",
    "                scores = cv_one_class_classification(model=model, data=X, labels=y, n_splits=CV_n_splits)\n",
    "                model_class_results.append([params, scores])\n",
    "            except ValueError: \n",
    "                print('non supported combination of parameters')\n",
    "                model_class_results.append(['non-valid',0])\n",
    "            print('done ', i , ' out of ',n_param_combinations, ' combinations')\n",
    "            i+=1\n",
    "        # put model class results into dictionary\n",
    "        results[model_class_name] = model_class_results\n",
    "        print(model_class_name, ' done')\n",
    "    \n",
    "    # Linear SVC ----------------------------------------------------------------------------------------------\n",
    "    if 'LinearSVC' in models.keys(): \n",
    "        # model class\n",
    "        from sklearn.svm import LinearSVC\n",
    "        model_class_name = 'LinearSVC'\n",
    "        model_class = LinearSVC()\n",
    "        #create list for model class results\n",
    "        model_class_results = []\n",
    "        # get number of combinations\n",
    "        n_param_combinations = models.get(model_class_name)\n",
    "        # parameter grid for the model class\n",
    "        param_grid ={\n",
    "            'penalty': ['l1','l2'],\n",
    "            'loss' : ['squared_hinge','hinge'],\n",
    "            'dual' : [False],                   #according to scikit-learn better False if n_samples > n_features\n",
    "            'tol' : [0.0001],\n",
    "            'C': [1.0 , 2.0 , 5.0 , 10.0, 0.5, 0.2],\n",
    "            #'multi_class' : ['ovr', 'crammer_singer'],\n",
    "            'fit_intercept' : [False],           #since data is assumed to be already centered \n",
    "            #'intercept_scaling'=[1], \n",
    "            'class_weight':['balanced'], \n",
    "            #'verbose'=[0], \n",
    "            #'random_state'=None, \n",
    "            'max_iter' : [1000],\n",
    "            'verbose' : [verbose], \n",
    "            #'n_components' : [1300, 1100, 1000,  800, 600, 500, 300, 200, 150]\n",
    "            }\n",
    "        #create list of parameter combinations\n",
    "        p_grids = list(ParameterGrid(param_grid))\n",
    "        total_combinations = len(p_grids)\n",
    "        if n_param_combinations >= total_combinations: \n",
    "            print('trying all possible combiations of parameters for ',model_class_name)\n",
    "        else: \n",
    "            print('trying ', n_param_combinations,' possible combiations of parameters for ',model_class_name)\n",
    "        # loop over parameter combinations\n",
    "        i = 1\n",
    "        while (i <= n_param_combinations) and (i <= total_combinations):\n",
    "            try:\n",
    "                params = random.choice(p_grids)\n",
    "                print(params)\n",
    "                p_grids.remove(params)\n",
    "                pipe = Pipeline([('imputing',SimpleImputer(strategy=impute_strategy)),('scaling', StandardScaler()),('model_class', model_class)])\n",
    "                pipe_params = make_pipe_model_params(params)\n",
    "                model = pipe.set_params(**pipe_params)\n",
    "                scores = cv_one_class_classification(model=model, data=X, labels=y, n_splits=CV_n_splits)\n",
    "                model_class_results.append([params, scores])\n",
    "            except ValueError: \n",
    "                print('non supported combination of parameters')\n",
    "                model_class_results.append(['non-valid',0])\n",
    "            print('done ', i , ' out of ',n_param_combinations, ' combinations')\n",
    "            i+=1\n",
    "        # put model class results into dictionary\n",
    "        results[model_class_name] = model_class_results\n",
    "        print(model_class_name, ' done')\n",
    "        \n",
    "    # SVC --------------------------------------------------------------------------------------------------- \n",
    "    if 'SVC' in models.keys(): \n",
    "        # model class\n",
    "        from sklearn.svm import SVC\n",
    "        model_class_name = 'SVC'\n",
    "        model_class = SVC()\n",
    "        #create list for model class results\n",
    "        model_class_results = []\n",
    "        # get number of combinations\n",
    "        n_param_combinations = models.get(model_class_name)\n",
    "        # parameter grid for the model class\n",
    "        param_grid ={\n",
    "            'C' : [1.0 , 2.0 , 5.0 , 10.0, 0.5, 0.2],\n",
    "            'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'coef0': [0.0, 0.1, 0.2, 0.5],\n",
    "            'class_weight':['balanced'], \n",
    "            'shrinking' : [True, False],\n",
    "            'verbose' : [verbose], \n",
    "            #'n_components' : [1300, 1100, 1000,  800, 600, 500, 300, 200, 150]\n",
    "            }\n",
    "        #create list of parameter combinations\n",
    "        p_grids = list(ParameterGrid(param_grid))\n",
    "        total_combinations = len(p_grids)\n",
    "        if n_param_combinations >= total_combinations: \n",
    "            print('trying all possible combiations of parameters for ',model_class_name)\n",
    "        else: \n",
    "            print('trying ', n_param_combinations,' possible combiations of parameters for ',model_class_name)     \n",
    "        # loop over parameter combinations\n",
    "        i = 1\n",
    "        while (i <= n_param_combinations) and (i <= total_combinations):\n",
    "            try:\n",
    "                params = random.choice(p_grids)\n",
    "                p_grids.remove(params)\n",
    "                pipe = Pipeline([('imputing',SimpleImputer(strategy=impute_strategy)),('scaling', StandardScaler()),('model_class', model_class)])\n",
    "                pipe_params = make_pipe_model_params(params)\n",
    "                model = pipe.set_params(**pipe_params)\n",
    "                scores = cv_one_class_classification(model=model, data=X, labels=y, n_splits=CV_n_splits)\n",
    "                model_class_results.append([params, scores])\n",
    "            except ValueError: \n",
    "                print('non supported combination of parameters')\n",
    "                model_class_results.append(['non-valid', 0])\n",
    "            print('done ', i , ' out of ',n_param_combinations, ' combinations')\n",
    "            i+=1\n",
    "        # put model class results into dictionary\n",
    "        results[model_class_name] = model_class_results\n",
    "        print(model_class_name, ' done')\n",
    "    \n",
    "    # Extra Trees classifier ------------------------------------------------------------------------------------\n",
    "    if 'ExtraTreesClassifier' in models.keys(): \n",
    "        # model class \n",
    "        from sklearn.ensemble import ExtraTreesClassifier\n",
    "        model_class_name = 'ExtraTreesClassifier'\n",
    "        model_class = ExtraTreesClassifier()\n",
    "        #create list for model class results\n",
    "        model_class_results = []\n",
    "        # gte numbr of combinations\n",
    "        n_param_combinations = models.get(model_class_name)\n",
    "        # parameter grid for the model class\n",
    "        param_grid ={\n",
    "            'n_estimators' : [200, 250, 300],\n",
    "            'criterion' : ['gini', 'entropy'],\n",
    "            'min_samples_split' : [2, 3, 4],\n",
    "            'bootstrap' : [True, False],\n",
    "            'class_weight' : ['balanced', 'balanced_subsample'],\n",
    "            'verbose' : [verbose],\n",
    "            'n_jobs' : [3]\n",
    "            }\n",
    "        #create list of parameter combinations\n",
    "        p_grids = list(ParameterGrid(param_grid))\n",
    "        total_combinations = len(p_grids)\n",
    "        if n_param_combinations >= total_combinations: \n",
    "            print('trying all possible combiations of parameters for ',model_class_name)\n",
    "        else: \n",
    "            print('trying ', n_param_combinations,' possible combiations of parameters for ',model_class_name)    \n",
    "        # loop over parameter combinations\n",
    "        i = 1\n",
    "        while (i <= n_param_combinations) and (i <= total_combinations):\n",
    "            try:\n",
    "                params = random.choice(p_grids)\n",
    "                p_grids.remove(params)\n",
    "                pipe = Pipeline([('imputing',SimpleImputer(strategy=impute_strategy)),('scaling', StandardScaler()), ('model_class', model_class)])\n",
    "                pipe_params = make_pipe_model_params(params)\n",
    "                model = pipe.set_params(**pipe_params)\n",
    "                scores = cv_one_class_classification(model=model, data=X, labels=y, n_splits=CV_n_splits)\n",
    "                model_class_results.append([params, scores])\n",
    "            except ValueError: \n",
    "                print('non supported combination of parameters')\n",
    "                model_class_results.append(['non-valid', 0])\n",
    "            print('done ', i , ' out of ',n_param_combinations, ' combinations')\n",
    "            i+=1\n",
    "        # put model class results into dictionary\n",
    "        results[model_class_name] = model_class_results\n",
    "        print(model_class_name, ' done')\n",
    "        \n",
    "    # Random Forest classifier ----------------------------------------------------------------------------------\n",
    "    if 'RandomForestClassifier' in models.keys(): \n",
    "        # import model class \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model_class_name = 'RandomForestClassifier'\n",
    "        model_class = RandomForestClassifier()\n",
    "        #create list for model class results\n",
    "        model_class_results = []\n",
    "        # gte numbr of combinations\n",
    "        n_param_combinations = models.get(model_class_name)\n",
    "        # parameter grid for the model class\n",
    "        param_grid ={\n",
    "            'n_estimators' : [150, 175, 200, 250],\n",
    "            'criterion' : ['gini', 'entropy'],\n",
    "            'min_samples_split' : [2, 3,4],\n",
    "            'bootstrap' : [True, False],\n",
    "            'class_weight' : ['balanced', 'balanced_subsample'],\n",
    "            'verbose' : [verbose],\n",
    "            'n_jobs' : [3]\n",
    "            }\n",
    "        #create list of parameter combinations\n",
    "        p_grids = list(ParameterGrid(param_grid))\n",
    "        total_combinations = len(p_grids)\n",
    "        if n_param_combinations >= total_combinations: \n",
    "            print('trying all possible combiations of parameters for ',model_class_name)\n",
    "        else: \n",
    "            print('trying ', n_param_combinations,' possible combiations of parameters for ',model_class_name)\n",
    "        # loop over parameter combinations\n",
    "        i = 1\n",
    "        while (i <= n_param_combinations) and (i <= total_combinations):\n",
    "            try:\n",
    "                params = random.choice(p_grids)\n",
    "                p_grids.remove(params)\n",
    "                pipe = Pipeline([('imputing',SimpleImputer(strategy=impute_strategy)),('scaling', StandardScaler()), ('model_class', model_class)])\n",
    "                pipe_params = make_pipe_model_params(params)\n",
    "                model = pipe.set_params(**pipe_params)\n",
    "                scores = cv_one_class_classification(model=model, data=X, labels=y, n_splits=CV_n_splits)\n",
    "                model_class_results.append([params, scores])\n",
    "            except ValueError: \n",
    "                print('non supported combination of parameters')\n",
    "                model_class_results.append(['non-valid', 0])\n",
    "            print('done ', i , ' out of ',n_param_combinations, ' combinations')\n",
    "            i+=1 \n",
    "        # put model class results into dictionary\n",
    "        results[model_class_name] = model_class_results\n",
    "        print(model_class_name, ' done')\n",
    "    \n",
    "    # AdaBoost classifier -----------------------------------------------------------------------------------\n",
    "    if 'AdaBoostClassifier' in models.keys(): \n",
    "        # import model class \n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "        model_class_name = 'AdaBoostClassifier'\n",
    "        model_class = AdaBoostClassifier()\n",
    "        #create list for model class results\n",
    "        model_class_results = []\n",
    "        #get number of combinations\n",
    "        n_param_combinations = models.get(model_class_name)\n",
    "        # parameter grid for the model class\n",
    "        param_grid ={\n",
    "            'base_estimator' : [DecisionTreeClassifier(max_depth=1), DecisionTreeClassifier(max_depth=2)],\n",
    "            'n_estimators' : [75, 100, 150],\n",
    "            'learning_rate' : [1.0, 1.5, 2.0, 1.75]\n",
    "            }\n",
    "        #create list of parameter combinations\n",
    "        p_grids = list(ParameterGrid(param_grid))\n",
    "        total_combinations = len(p_grids)\n",
    "        if n_param_combinations >= total_combinations: \n",
    "            print('trying all possible combiations of parameters for ',model_class_name)\n",
    "        else: \n",
    "            print('trying ', n_param_combinations,' possible combiations of parameters for ',model_class_name)       \n",
    "        # loop over parameter combinations\n",
    "        i = 1\n",
    "        while (i <= n_param_combinations) and (i <= total_combinations):\n",
    "            try:\n",
    "                params = random.choice(p_grids)\n",
    "                p_grids.remove(params)\n",
    "                pipe = Pipeline([('imputing',SimpleImputer(strategy=impute_strategy)),('scaling', StandardScaler()), ('model_class', model_class)])\n",
    "                pipe_params = make_pipe_model_params(params)\n",
    "                model = pipe.set_params(**pipe_params)\n",
    "                scores = cv_one_class_classification(model=model, data=X, labels=y, n_splits=CV_n_splits)\n",
    "                model_class_results.append([params, scores])\n",
    "            except ValueError: \n",
    "                print('non supported combination of parameters')\n",
    "                model_class_results.append(['non-valid', 0])\n",
    "            print('done ', i , ' out of ',n_param_combinations, ' combinations')\n",
    "            i+=1\n",
    "        # put model class results into dictionary\n",
    "        results[model_class_name] = model_class_results\n",
    "        print(model_class_name, ' done')\n",
    "    # XGBoost classifier -----------------------------------------------------------------------------------\n",
    "    if 'XGBClassifier' in models.keys(): \n",
    "        # import model class \n",
    "        from xgboost import XGBClassifier\n",
    "        model_class_name = 'XGBClassifier'\n",
    "        model_class = XGBClassifier()\n",
    "        #create list for model class results\n",
    "        model_class_results = []\n",
    "        #get number of combinations\n",
    "        n_param_combinations = models.get(model_class_name)\n",
    "        # parameter grid for the model class\n",
    "        param_grid ={\n",
    "            'max_depth' : [1,2,3], \n",
    "            'learning_rate' : [0.1,0.2,0.3,0.4],\n",
    "            'n_estimators' : [150,200,250, 300],\n",
    "            'verbosity' : [verbose],\n",
    "            'objective' : ['binary:logistic'],\n",
    "            'n_jobs' : [3], \n",
    "            'gamma' : [0, 0.1, 0.5, 1],\n",
    "            'reg_alpha' : [0, 0.5, 1],\n",
    "            'reg_alpha' : [0, 0.5, 1, 2],\n",
    "            'scale_pos_weight': [6.6]     # computed as negative samples/positive samples\n",
    "            }\n",
    "        #create list of parameter combinations\n",
    "        p_grids = list(ParameterGrid(param_grid))\n",
    "        total_combinations = len(p_grids)\n",
    "        if n_param_combinations >= total_combinations: \n",
    "            print('trying all possible combiations of parameters for ',model_class_name)\n",
    "        else: \n",
    "            print('trying ', n_param_combinations,' possible combiations of parameters for ',model_class_name)       \n",
    "        # loop over parameter combinations\n",
    "        i = 1\n",
    "        while (i <= n_param_combinations) and (i <= total_combinations):\n",
    "            try:\n",
    "                params = random.choice(p_grids)\n",
    "                p_grids.remove(params)\n",
    "                pipe = Pipeline([('imputing',SimpleImputer(strategy=impute_strategy)),('scaling', StandardScaler()), ('model_class', model_class)])\n",
    "                pipe_params = make_pipe_model_params(params)\n",
    "                model = pipe.set_params(**pipe_params)\n",
    "                scores = cv_one_class_classification(model=model, data=X, labels=y, n_splits=CV_n_splits)\n",
    "                model_class_results.append([params, scores])\n",
    "            except ValueError: \n",
    "                print('non supported combination of parameters')\n",
    "                model_class_results.append(['non-valid', 0])\n",
    "            print('done ', i , ' out of ',n_param_combinations, ' combinations')\n",
    "            i+=1\n",
    "        # put model class results into dictionary\n",
    "        results[model_class_name] = model_class_results\n",
    "        print(model_class_name, ' done') \n",
    "    \n",
    "    for model_class_name in results.keys(): \n",
    "        means_b_accuracy = []\n",
    "        means_f1_score = []\n",
    "        medians_b_accuracy  = []\n",
    "        medians_f1_score = []\n",
    "        for params, scores in results[model_class_name]:\n",
    "            if params != 'non-valid':\n",
    "                means_b_accuracy.append(np.mean(np.asarray(scores['balanced_accuracy'])))\n",
    "                means_f1_score.append(np.mean(np.asarray(scores['f1_score'])))\n",
    "                medians_b_accuracy.append(np.median(np.asarray(scores['balanced_accuracy'])))\n",
    "                medians_f1_score.append(np.median(np.asarray(scores['f1_score'])))\n",
    "            else:\n",
    "                means_b_accuracy.append(-1)\n",
    "                means_f1_score.append(-1)\n",
    "                medians_b_accuracy.append(-1)\n",
    "                medians_f1_score.append(-1)\n",
    "                \n",
    "        index_b_accuracy = np.argmax(np.asarray(means_b_accuracy))\n",
    "        index_f1_score = np.argmax(np.asarray(means_f1_score))\n",
    "        \n",
    "        best_score_b_acc_mean = np.asarray(means_b_accuracy)[index_b_accuracy]\n",
    "        best_score_f1_score_mean = np.asarray(means_f1_score)[index_f1_score]\n",
    "        \n",
    "        best_score_b_acc_median = np.asarray(medians_b_accuracy)[index_b_accuracy]\n",
    "        best_score_f1_score_median = np.asarray(medians_f1_score)[index_f1_score]\n",
    "        \n",
    "        best_params_b_acc = results[model_class_name][index_b_accuracy][0]\n",
    "        best_params_f_score = results[model_class_name][index_f1_score][0]\n",
    "        \n",
    "        \n",
    "        bests[model_class_name] ={\n",
    "            'balanced_accuracy':{'mean':best_score_b_acc_mean,'median': best_score_b_acc_median,'parameters':best_params_b_acc, \n",
    "                        'f1_score': {'mean':np.asarray(means_f1_score)[index_b_accuracy],'median':np.asarray(medians_f1_score)[index_b_accuracy]} },\n",
    "\n",
    "            'f1_score':{'mean':best_score_f1_score_mean,'median': best_score_f1_score_median,'parameters':best_params_f_score, \n",
    "                        'balanced_accuracy': {'mean':np.asarray(means_b_accuracy)[index_f1_score],'median':np.asarray(medians_b_accuracy)[index_f1_score]} },\n",
    "\n",
    "        }\n",
    "    return results, bests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import data non-extensive (only clusters and ratios)\n",
    "eeg_clusters = create_dataset_eeg(SCORE = 'Age', clusters = True, ratios = True)\n",
    "\n",
    "#drop samples with Nan in 'DX_01_Cat'\n",
    "eeg_clusters = drop_nans_based_on_column(eeg_clusters, 'DX_01_Cat')\n",
    "\n",
    "eeg_clusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show statistics for DISEASE CATEGORIES 'DX_01_Cat' \n",
    "get_statistics_column(eeg_clusters, 'DX_01_Cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 'Neurodevelopmental Disorders' is by far the most present category, so we do further statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurodevelopmental Disorders data stats\n",
    "#create dataset for only ND patients and drop patients with nulls in 'DX_01_Sub', 'DX_01'\n",
    "ND = eeg_clusters.groupby('DX_01_Cat').get_group('Neurodevelopmental Disorders')\n",
    "ND = drop_nans_based_on_column(ND, 'DX_01_Sub')\n",
    "ND = drop_nans_based_on_column(ND, 'DX_01')\n",
    "\n",
    "# show stats for subcategories in ND data\n",
    "get_statistics_column(ND, 'DX_01_Sub')\n",
    "\n",
    "# show stats for 'DX_01' in ND data\n",
    "get_statistics_column(ND, 'DX_01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some takeaway points are: \n",
    "- Neurodevelopmental Disorders(ND) data not have Nans in Subcategories or Diagnoses and are a lot (833/1305)\n",
    "- ADHD Subcategory is 577 samples out of 1305 (around 0.44) --> can try to predict that against all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting ADHD (as a Subcategory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicting ADHD subcat: preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create list of subcategories\n",
    "subcats = list(set(eeg_clusters['DX_01_Sub'].values))\n",
    "print(subcats)\n",
    "\n",
    "# create dict to later map ADHD-->1 vs all other subcats-->0\n",
    "d = { \n",
    " 'Attention-Deficit/Hyperactivity Disorder' : 1\n",
    "}\n",
    "subcats.remove( 'Attention-Deficit/Hyperactivity Disorder')\n",
    "for subcat in subcats: \n",
    "    d[subcat]=0\n",
    "print(d)\n",
    "\n",
    "#create ADHD dataset\n",
    "ADHD = eeg_clusters.copy()\n",
    "ADHD['DX_01_Sub'] = ADHD['DX_01_Sub'].map(d)\n",
    "\n",
    "# remove othe diagnoses-related columns\n",
    "ADHD = ADHD.drop(['DX_01_Cat', 'DX_01'], axis=1)\n",
    "\n",
    "#rename 'DX_01_Sub'-->'label' column\n",
    "ADHD = ADHD.rename(columns={'DX_01_Sub': 'label'})\n",
    "\n",
    "ADHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import behavioral data to later get SWAN data, that will be used as a 'baseline' for predicting ADHD\n",
    "behavioral = pd.read_csv('data/Behavioral/cleaned/HBNFinalSummaries.csv')\n",
    "behavioral\n",
    "\n",
    "# get SWAN columns\n",
    "swan_cols = list(behavioral.filter(like='SWAN').columns)\n",
    "swan_cols\n",
    "\n",
    "#get SWAN data\n",
    "SWAN = behavioral[ ['EID','Sex'] + list(behavioral.filter(like='SWAN').columns)]\n",
    "#rename id column\n",
    "SWAN = SWAN.rename(columns={'EID': 'id'})\n",
    "\n",
    "SWAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join on patient id and get ADHD_SWAN dataset\n",
    "\n",
    "try: \n",
    "    ADHD_SWAN = pd.merge(SWAN, ADHD, on='id', how='inner',validate='one_to_one')\n",
    "except pd.errors.MergeError: \n",
    "    SWAN = SWAN.drop_duplicates(subset=['id'], keep='first')\n",
    "    ADHD = ADHD.drop_duplicates(subset=['id'], keep='first')\n",
    "    ADHD_SWAN = pd.merge(SWAN, ADHD, on='id', how='inner',validate='one_to_one')\n",
    "\n",
    "ADHD_SWAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distributions of SWAN columns values\n",
    "\n",
    "col = 'SWAN_IN_Avg'\n",
    "plt.title(col)\n",
    "plt.hist(ADHD_SWAN[col], bins=50)\n",
    "plt.xlabel(col)\n",
    "plt.ylabel('n° of patients')\n",
    "plt.show()\n",
    "\n",
    "col = 'SWAN_HY_Avg'\n",
    "plt.title(col)\n",
    "plt.xlabel(col)\n",
    "plt.ylabel('n° of patients')\n",
    "plt.hist(ADHD_SWAN[col], bins=50, color='green')\n",
    "plt.show()\n",
    "\n",
    "col = 'SWAN_Avg'\n",
    "plt.title(col)\n",
    "plt.xlabel(col)\n",
    "plt.ylabel('n° of patients')\n",
    "plt.hist(ADHD_SWAN[col], bins=50, color='orange')\n",
    "plt.show()\n",
    "\n",
    "# correlation matrix for SWAN columns\n",
    "ax = plt.axes()\n",
    "sns.heatmap(ADHD_SWAN[swan_cols].corr(),annot=True, cmap=sns.color_palette(\"coolwarm\", 4), ax=ax)\n",
    "ax.set_title('Correlation matrix SWAN columns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWAN_Avg is a lor correlated (about 0.9) to the other two SWAN columns. Better drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop SWAN Avg\n",
    "ADHD_SWAN = ADHD_SWAN.drop('SWAN_Avg',axis=1)\n",
    "swan_cols.remove('SWAN_Avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ADHD_SWAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with nans \n",
    "\n",
    "for column in ADHD_SWAN.columns: \n",
    "    nan_sum = ADHD_SWAN.loc[:, column].isna().sum()\n",
    "    if nan_sum >= 1: \n",
    "        print(column, '\\t', nan_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicting ADHD subact : modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list for test ids\n",
    "test_ids = pd.read_csv('data/test_IDS.csv')\n",
    "test_ids_a = test_ids['ID'].values\n",
    "test_ids_l = list(test_ids_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data ADHD_SWAN\n",
    "ADHD_SWAN_test = ADHD_SWAN.loc[ADHD_SWAN['id'].isin(test_ids_l)]\n",
    "ADHD_SWAN_train = ADHD_SWAN.loc[~ADHD_SWAN['id'].isin(test_ids_l)]\n",
    "\n",
    "#pop id and label column train\n",
    "id_column_train = ADHD_SWAN_train.pop('id')\n",
    "label_train = ADHD_SWAN_train.pop('label')\n",
    "\n",
    "#pop id and label column test \n",
    "id_column_test = ADHD_SWAN_test.pop('id')\n",
    "label_test = ADHD_SWAN_test.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dealing with Nans in training data\n",
    "for column in ADHD_SWAN_train.columns: \n",
    "    nan_sum = ADHD_SWAN_train.loc[:, column].isna().sum()\n",
    "    if nan_sum >= 1: \n",
    "        print(column, '\\t', nan_sum)\n",
    "'''\n",
    "# imputing missing values: strategy= median\n",
    "from sklearn.impute import SimpleImputer\n",
    "impute_strategy = 'median'\n",
    "imputer = SimpleImputer(strategy=impute_strategy)\n",
    "\n",
    "imputer.fit(ADHD_SWAN_train)\n",
    "ADHD_SWAN_train = pd.DataFrame(imputer.transform(ADHD_SWAN_train), index=ADHD_SWAN_train.index, columns=ADHD_SWAN_train.columns)\n",
    "ADHD_SWAN_test = pd.DataFrame(imputer.transform(ADHD_SWAN_test), index=ADHD_SWAN_test.index, columns=ADHD_SWAN_test.columns)\n",
    "\n",
    "# THIS PART IS REMOVED SINCE THE SCALING IS DONE DIRECTLY IN THE PIPELINE\n",
    "\n",
    "# scaling data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(ADHD_SWAN_train)\n",
    "\n",
    "ADHD_SWAN_train = pd.DataFrame(scaler.transform(ADHD_SWAN_train), index=ADHD_SWAN_train.index, columns=ADHD_SWAN_train.columns)\n",
    "ADHD_SWAN_test = pd.DataFrame(scaler.transform(ADHD_SWAN_test), index=ADHD_SWAN_test.index, columns=ADHD_SWAN_test.columns)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predicting ADHD using only SWAN (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get train and test dataset SWAN columns\n",
    "X_swan_train = ADHD_SWAN_train[swan_cols]  #only SWAN columns dataset\n",
    "X_swan_test = ADHD_SWAN_test[swan_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SWAN impute strategy='median''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PIPELINE 1\n",
    "\n",
    "imp_strategy = 'median'\n",
    "\n",
    "models = {\n",
    "    'RidgeClassifier' : 10, \n",
    "    'LinearSVC': 10,\n",
    "    'SVC' : 15,\n",
    "    'RandomForestClassifier' : 15, \n",
    "    'ExtraTreesClassifier' : 15, \n",
    "    'AdaBoostClassifier' : 15, \n",
    "    'XGBClassifier' : 15\n",
    "}\n",
    "results_SWAN, bests_SWAN = one_class_classify_CV(X=X_swan_train, y=label_train, models=models, CV_n_splits=5, verbose=0, impute_strategy=imp_strategy)\n",
    "\n",
    "# PIPELINE 2 \n",
    "# decide to go for the accuracy metric of the three\n",
    "selected_metric = 'balanced_accuracy'\n",
    "\n",
    "model_classes_dict = {\n",
    "    'RidgeClassifier' : RidgeClassifier(), \n",
    "    'LinearSVC': LinearSVC(),\n",
    "    'SVC' : SVC(),\n",
    "    'RandomForestClassifier' : RandomForestClassifier(), \n",
    "    'ExtraTreesClassifier' : ExtraTreesClassifier(), \n",
    "    'AdaBoostClassifier' : AdaBoostClassifier(), \n",
    "    'XGBClassifier' : XGBClassifier()\n",
    "}\n",
    "\n",
    "for model_class_name, model_class in model_classes_dict.items():\n",
    "    parameters = bests_SWAN[model_class_name][selected_metric]['parameters']\n",
    "    estimator = model_class.set_params(**parameters)\n",
    "    model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('estimator', estimator)])\n",
    "    results = cv_one_class_classification(model, X_swan_train, label_train, n_splits=5)\n",
    "    b_acc_med = np.median(np.array(results['balanced_accuracy']))\n",
    "    f1_score_med = np.median(np.array(results['f1_score']))\n",
    "    b_acc_m = np.array(results['balanced_accuracy']).mean()\n",
    "    f1_score_m = np.array(results['f1_score']).mean()\n",
    "    print('\\n\\n',\n",
    "          model_class_name, '\\n\\t balanced_accuracy:  mean {} \\t median {}' \n",
    "          '\\n f1_score:  mean {} \\t median {} \\n\\n'.format(b_acc_m, b_acc_med, f1_score_m, f1_score_med))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PIPELINE 3\n",
    "# select best model (manually) and predict on test set \n",
    "\n",
    "best_model_class = 'XGBClassifier'\n",
    "\n",
    "best_model_parameters = bests_SWAN[best_model_class][selected_metric]['parameters']\n",
    "best_estimator = model_classes_dict[best_model_class].set_params(**best_model_parameters)\n",
    "best_model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('model', best_estimator)])\n",
    "\n",
    "best_model.fit(X_swan_train, label_train)\n",
    "best_model_preds = best_model.predict(X_swan_test)\n",
    "\n",
    "best_model_test_b_accuracy = balanced_accuracy_score(label_test, best_model_preds)\n",
    "best_model_test_f1_score= f1_score(label_test, best_model_preds)\n",
    "\n",
    "print('\\n\\n Best model results test set: \\n balanced_accuracy {} \\t f1_score {} \\n'.format(best_model_test_b_accuracy, best_model_test_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SWAN: impute strategy='mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 1\n",
    "\n",
    "imp_strategy = 'mean'\n",
    "\n",
    "models = {\n",
    "    'RidgeClassifier' : 10, \n",
    "    'LinearSVC': 10,\n",
    "    'SVC' : 15,\n",
    "    'RandomForestClassifier' : 15, \n",
    "    'ExtraTreesClassifier' : 15, \n",
    "    'AdaBoostClassifier' : 15, \n",
    "    'XGBClassifier' : 15\n",
    "}\n",
    "results_SWAN, bests_SWAN = one_class_classify_CV(X=X_swan_train, y=label_train, models=models, CV_n_splits=5, verbose=0, impute_strategy=imp_strategy)\n",
    "\n",
    "# PIPELINE 2 \n",
    "# decide to go for the accuracy metric of the three\n",
    "selected_metric = 'balanced_accuracy'\n",
    "\n",
    "model_classes_dict = {\n",
    "    'RidgeClassifier' : RidgeClassifier(), \n",
    "    'LinearSVC': LinearSVC(),\n",
    "    'SVC' : SVC(),\n",
    "    'RandomForestClassifier' : RandomForestClassifier(), \n",
    "    'ExtraTreesClassifier' : ExtraTreesClassifier(), \n",
    "    'AdaBoostClassifier' : AdaBoostClassifier(), \n",
    "    'XGBClassifier' : XGBClassifier()\n",
    "}\n",
    "\n",
    "for model_class_name, model_class in model_classes_dict.items():\n",
    "    parameters = bests_SWAN[model_class_name][selected_metric]['parameters']\n",
    "    estimator = model_class.set_params(**parameters)\n",
    "    model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('estimator', estimator)])\n",
    "    results = cv_one_class_classification(model, X_swan_train, label_train, n_splits=5)\n",
    "    b_acc_med = np.median(np.array(results['balanced_accuracy']))\n",
    "    f1_score_med = np.median(np.array(results['f1_score']))\n",
    "    b_acc_m = np.array(results['balanced_accuracy']).mean()\n",
    "    f1_score_m = np.array(results['f1_score']).mean()\n",
    "    print('\\n\\n',\n",
    "          model_class_name, '\\n\\t balanced_accuracy:  mean {} \\t median {}' \n",
    "          '\\n\\t f1_score:  mean {} \\t median {} \\n\\n'.format(b_acc_m, b_acc_med, f1_score_m, f1_score_med))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PIPELINE 3\n",
    "# select best model (manually) and predict on test set \n",
    "\n",
    "best_model_class = 'LinearSVC'\n",
    "\n",
    "best_model_parameters = bests_SWAN[best_model_class][selected_metric]['parameters']\n",
    "best_estimator = model_classes_dict[best_model_class].set_params(**best_model_parameters)\n",
    "best_model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('model', best_estimator)])\n",
    "\n",
    "best_model.fit(X_swan_train, label_train)\n",
    "best_model_preds = best_model.predict(X_swan_test)\n",
    "\n",
    "best_model_test_b_accuracy = balanced_accuracy_score(label_test, best_model_preds)\n",
    "best_model_test_f1_score= f1_score(label_test, best_model_preds)\n",
    "\n",
    "print('\\n\\n Best model results test set: \\n balanced_accuracy {} \\t f1_score {} \\n'.format(best_model_test_b_accuracy, best_model_test_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predicting using only EEG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test dataset EEG data for ADHD\n",
    "X_EEGxADHD_train = ADHD_SWAN_train.drop(['Sex','SWAN_IN_Avg','SWAN_HY_Avg','Age'], axis=1)\n",
    "X_EEGxADHD_test = ADHD_SWAN_test.drop(['Sex','SWAN_IN_Avg','SWAN_HY_Avg','Age'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### predicting using only EEG: impute strategy 'median'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 1\n",
    "\n",
    "imp_strategy = 'median'\n",
    "\n",
    "models = {\n",
    "    'RidgeClassifier' : 10, \n",
    "    'LinearSVC': 15,\n",
    "    'SVC' : 20,\n",
    "    'RandomForestClassifier' : 20, \n",
    "    'ExtraTreesClassifier' : 20, \n",
    "    'AdaBoostClassifier' : 20, \n",
    "    'XGBClassifier' : 20\n",
    "}\n",
    "results_EEGxADHD, bests_EEGxADHD = one_class_classify_CV(X=X_EEGxADHD_train, y=label_train, models=models, CV_n_splits=5, verbose=0, impute_strategy=imp_strategy)\n",
    "\n",
    "# PIPELINE 2 \n",
    "# decide to go for the accuracy metric of the three\n",
    "selected_metric = 'balanced_accuracy'\n",
    "\n",
    "model_classes_dict = {\n",
    "    'RidgeClassifier' : RidgeClassifier(), \n",
    "    'LinearSVC': LinearSVC(),\n",
    "    'SVC' : SVC(),\n",
    "    'RandomForestClassifier' : RandomForestClassifier(), \n",
    "    'ExtraTreesClassifier' : ExtraTreesClassifier(), \n",
    "    'AdaBoostClassifier' : AdaBoostClassifier(), \n",
    "    'XGBClassifier' : XGBClassifier()\n",
    "}\n",
    "\n",
    "for model_class_name, model_class in model_classes_dict.items():\n",
    "    parameters = bests_EEGxADHD[model_class_name][selected_metric]['parameters']\n",
    "    estimator = model_class.set_params(**parameters)\n",
    "    model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('estimator', estimator)])\n",
    "    results = cv_one_class_classification(model, X_EEGxADHD_train, label_train, n_splits=5)\n",
    "    b_acc_med = np.median(np.array(results['balanced_accuracy']))\n",
    "    f1_score_med = np.median(np.array(results['f1_score']))\n",
    "    b_acc_m = np.array(results['balanced_accuracy']).mean()\n",
    "    f1_score_m = np.array(results['f1_score']).mean()\n",
    "    print('\\n\\n',\n",
    "          model_class_name, '\\n\\t balanced_accuracy:  mean {} \\t median {}' \n",
    "          '\\n\\t f1_score:  mean {} \\t median {} \\n\\n'.format(b_acc_m, b_acc_med, f1_score_m, f1_score_med))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PIPELINE 3\n",
    "# select best model (manually) and predict on test set \n",
    "\n",
    "best_model_class = 'XGBClassifier'\n",
    "\n",
    "best_model_parameters = bests_EEGxADHD[best_model_class][selected_metric]['parameters']\n",
    "best_estimator = model_classes_dict[best_model_class].set_params(**best_model_parameters)\n",
    "best_model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('model', best_estimator)])\n",
    "\n",
    "best_model.fit(X_EEGxADHD_train, label_train)\n",
    "best_model_preds = best_model.predict(X_EEGxADHD_test)\n",
    "\n",
    "best_model_test_b_accuracy = balanced_accuracy_score(label_test, best_model_preds)\n",
    "best_model_test_f1_score= f1_score(label_test, best_model_preds)\n",
    "print('\\n\\n Best model results test set: \\n balanced_accuracy {} \\t f1_score {} \\n'.format(best_model_test_b_accuracy, best_model_test_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importances for XGBoost model for EEG \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.set_params(**(bests_EEGxADHD['XGBClassifier']['accuracy']['parameters']))\n",
    "\n",
    "model.fit(X_EEGxADHD, label_column)\n",
    "\n",
    "plot_importance(model)\n",
    "plt.show()\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "n_features = 5\n",
    "n_most_imp_feat = feature_importances.argsort()[-n_features:][::-1]\n",
    "\n",
    "for i, feature in enumerate(X_EEGxADHD.columns): \n",
    "    if i in n_most_imp_feat: \n",
    "        print(feature, feature_importances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predicting using SWAN + EEG + Sex + Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### predicting using impute strategy 'median'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 1\n",
    "\n",
    "imp_strategy = 'median'\n",
    "\n",
    "models = {\n",
    "    'RidgeClassifier' : 10, \n",
    "    'LinearSVC': 15,\n",
    "    'SVC' : 20,\n",
    "    'RandomForestClassifier' : 20, \n",
    "    'ExtraTreesClassifier' : 20, \n",
    "    'AdaBoostClassifier' : 20, \n",
    "    'XGBClassifier' : 25\n",
    "}\n",
    "results_EEGSWAN, bests_EEGSWAN = one_class_classify_CV(X=ADHD_SWAN_train, y=label_train, models=models, CV_n_splits=5, verbose=0, impute_strategy=imp_strategy)\n",
    "\n",
    "# PIPELINE 2 \n",
    "# decide to go for the accuracy metric of the three\n",
    "selected_metric = 'balanced_accuracy'\n",
    "\n",
    "model_classes_dict = {\n",
    "    \n",
    "    'RidgeClassifier' : RidgeClassifier(), \n",
    "    'LinearSVC': LinearSVC(),\n",
    "    'SVC' : SVC(),\n",
    "    'RandomForestClassifier' : RandomForestClassifier(), \n",
    "    'ExtraTreesClassifier' : ExtraTreesClassifier(), \n",
    "    'AdaBoostClassifier' : AdaBoostClassifier(), \n",
    "    'XGBClassifier' : XGBClassifier()\n",
    "}\n",
    "\n",
    "for model_class_name, model_class in model_classes_dict.items():\n",
    "    parameters = bests_EEGSWAN[model_class_name][selected_metric]['parameters']\n",
    "    estimator = model_class.set_params(**parameters)\n",
    "    model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('estimator', estimator)])\n",
    "    results = cv_one_class_classification(model, ADHD_SWAN_train, label_train, n_splits=5)\n",
    "    b_acc_med = np.median(np.array(results['balanced_accuracy']))\n",
    "    f1_score_med = np.median(np.array(results['f1_score']))\n",
    "    b_acc_m = np.array(results['balanced_accuracy']).mean()\n",
    "    f1_score_m = np.array(results['f1_score']).mean()\n",
    "    print('\\n\\n',\n",
    "          model_class_name, '\\n\\t balanced_accuracy:  mean {} \\t median {}' \n",
    "          '\\n\\t f1_score:  mean {} \\t median {} \\n\\n'.format(b_acc_m, b_acc_med, f1_score_m, f1_score_med))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PIPELINE 3\n",
    "# select best model (manually) and predict on test set \n",
    "\n",
    "best_model_class = 'XGBClassifier'\n",
    "\n",
    "best_model_parameters = bests_EEGSWAN[best_model_class][selected_metric]['parameters']\n",
    "best_estimator = model_classes_dict[best_model_class].set_params(**best_model_parameters)\n",
    "best_model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('model', best_estimator)])\n",
    "\n",
    "best_model.fit(ADHD_SWAN_train, label_train)\n",
    "best_model_preds = best_model.predict(ADHD_SWAN_test)\n",
    "\n",
    "\n",
    "best_model_test_b_accuracy = balanced_accuracy_score(label_test, best_model_preds)\n",
    "best_model_test_f1_score= f1_score(label_test, best_model_preds)\n",
    "print('\\n\\n Best model results test set: \\n balanced_accuracy {} \\t f1_score {} \\n'.format(best_model_test_b_accuracy, best_model_test_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get feature importances for XGBoost model for SWAN + EEG + Sex + Age \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "best_model.fit(ADHD_SWAN_train, label_train)\n",
    "\n",
    "feature_importances = best_model['model'].feature_importances_\n",
    "\n",
    "n_features = 10\n",
    "n_most_imp_feat = feature_importances.argsort()[-n_features:][::-1]\n",
    "\n",
    "for i, feature in enumerate(ADHD_SWAN.columns): \n",
    "    if i in n_most_imp_feat: \n",
    "        print(feature, feature_importances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for seemingly most relevant EEG features\n",
    "\n",
    "figure(figsize = (15,10))\n",
    "plt.scatter(ADHD_SWAN['eyesclosed_fooof_aperiodic_slope_lfront'], ADHD_SWAN['eyesclosed_fband_alpha_absmean_mfront'], alpha=0.2,\n",
    "            s=5, c=label_column, cmap='RdBu')\n",
    "plt.xlabel('eyesclosed_fooof_aperiodic_slope_lfront')\n",
    "plt.ylabel('eyesclosed_fband_alpha_absmean_mfront')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distributons for 'eyesclosed_ratios_lower2alpha_beta_lfront_mpari ' for positive vs negative samples\n",
    "\n",
    "col_name = 'eyesclosed_ratios_lower2alpha_beta_lfront_mpari'\n",
    "column = pd.DataFrame(ADHD_SWAN_train['eyesclosed_ratios_lower2alpha_beta_lfront_mpari'], index=ADHD_SWAN_train.index ,columns = ['eyesclosed_ratios_lower2alpha_beta_lfront_mpari'])\n",
    "column ['label'] = label_train\n",
    "\n",
    "column_pos = column.loc[column['label'] == 1]\n",
    "column_neg = column.loc[column['label'] == 0]\n",
    "\n",
    "\n",
    "figure(figsize = (11,8))\n",
    "plt.title('Patients with ADHD')\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('n° of patients')\n",
    "sns.distplot(column_pos[col_name], bins=50, norm_hist=True)\n",
    "plt.show()\n",
    "\n",
    "figure(figsize = (11,8))\n",
    "plt.title('Patients without ADHD')\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('n° of patients')\n",
    "sns.distplot(column_neg[col_name], bins=50, color='orange', norm_hist=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "figure(figsize = (11,8))\n",
    "#plt.title('Patients with ADHD')\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('n° of patients')\n",
    "sns.distplot(column_pos[col_name], bins=50, norm_hist=True)\n",
    "#plt.show()\n",
    "#plt.title('Patients without ADHD')\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('n° of patients')\n",
    "sns.distplot(column_neg[col_name], bins=50, color='orange', norm_hist=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if there is correlation between ratios and ADHD\n",
    "\n",
    "eeg_clusters_c = eeg_clusters.copy()\n",
    "# keep only ND and No Diagnoses (healthy)\n",
    "for row in eeg_clusters_c.index: \n",
    "    DX1Cat = eeg_clusters_c.loc[row,'DX_01_Cat'] \n",
    "    if DX1Cat != 'Neurodevelopmental Disorders' and DX1Cat != 'No Diagnosis Given':\n",
    "        eeg_clusters_c.drop(index=[row], inplace=True)\n",
    "\n",
    "for row in eeg_clusters_c.index: \n",
    "    DX1Cat = eeg_clusters_c.loc[row,'DX_01_Cat'] \n",
    "    DX1Sub = eeg_clusters_c.loc[row,'DX_01_Sub']\n",
    "    if DX1Cat == 'Neurodevelopmental Disorders' and DX1Sub != 'Attention-Deficit/Hyperactivity Disorder':\n",
    "        eeg_clusters_c.drop(index=[row], inplace=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eeg_clusters_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_ratios = ['eyesclosed_ratios_theta_lower2alpha_mfront_mpari', \n",
    "'eyesclosed_ratios_theta_lower2alpha_rpari_mpari', \n",
    "'eyesclosed_ratios_lower1alpha_lower2alpha_rfront_lfront',\n",
    "'eyesclosed_ratios_lower2alpha_beta_lfront_mpari',\n",
    "'eyesopen_ratios_theta_lower1alpha_rpari_lfront',\n",
    "'eyesopen_ratios_lower2alpha_beta_mfront_lpari']\n",
    "\n",
    "eeg_clusters_c['sum_sig_ratios'] = eeg_clusters_c[sig_ratios].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot distributons for 'eyesclosed_ratios_lower2alpha_beta_lfront_mpari ' healthy vs ADHD\n",
    "\n",
    "col_name = 'sum_sig_ratios'\n",
    "column = pd.DataFrame(eeg_clusters_c[col_name], index=eeg_clusters_c.index ,columns = [col_name])\n",
    "column ['label'] = eeg_clusters_c['DX_01_Cat']\n",
    "\n",
    "column_h = column.loc[column['label'] == 'No Diagnosis Given' ]\n",
    "column_adhd = column.loc[column['label'] == 'Neurodevelopmental Disorders']\n",
    "\n",
    "\n",
    "figure(figsize = (11,8))\n",
    "plt.title('Patients with ADHD')\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('n° of patients')\n",
    "sns.distplot(column_adhd[col_name], bins=1000, norm_hist=True)\n",
    "plt.show()\n",
    "\n",
    "figure(figsize = (11,8))\n",
    "plt.title('Healthy patients')\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('n° of patients')\n",
    "sns.distplot(column_h[col_name], bins=1000, color='orange', norm_hist=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "figure(figsize = (11,8))\n",
    "#plt.title('Patients with ADHD')\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('n° of patients')\n",
    "sns.distplot(column_adhd[col_name], bins=1000, norm_hist=True)\n",
    "#plt.show()\n",
    "#plt.title('Patients without ADHD')\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('n° of patients')\n",
    "sns.distplot(column_h[col_name], bins=1000, color='orange', norm_hist=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to aggregate ratios \n",
    "\n",
    "sum_ratios_vs_diag = eeg_clusters_c[['DX_01_Cat', 'sum_sig_ratios']]\n",
    "for index in sum_ratios_vs_diag.index:\n",
    "    if sum_ratios_vs_diag.loc[index, 'DX_01_Cat'] == 'Neurodevelopmental Disorders': \n",
    "        sum_ratios_vs_diag.loc[index, 'DX_01_Cat'] = 1\n",
    "    else: \n",
    "        sum_ratios_vs_diag.loc[index, 'DX_01_Cat'] = 0\n",
    "sum_ratios_vs_diag['DX_01_Cat'] = sum_ratios_vs_diag['DX_01_Cat'].astype('int64')\n",
    "\n",
    "\n",
    "print(sum_ratios_vs_diag)\n",
    "\n",
    "s_ratios_adhd =sum_ratios_vs_diag.loc[sum_ratios_vs_diag['DX_01_Cat'] == 1 ]['sum_sig_ratios'].values\n",
    "s_ratios_h =sum_ratios_vs_diag.loc[sum_ratios_vs_diag['DX_01_Cat'] == 0 ]['sum_sig_ratios'].values\n",
    "\n",
    "print(s_ratios_adhd.mean(),np.median(s_ratios_adhd),  s_ratios_h.mean(), np.median(s_ratios_h))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model predicts always the most frequent class\n",
    "\n",
    "#EEG\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "best_model_svc = SVC().set_params(**(bests_EEGxADHD['SVC']['accuracy']['parameters']))\n",
    "best_model_xgb = XGBClassifier().set_params(**(bests_EEGxADHD['XGBClassifier']['accuracy']['parameters']))\n",
    "best_model_et = ExtraTreesClassifier().set_params(**(bests_EEGxADHD['ExtraTreesClassifier']['accuracy']['parameters']))\n",
    "\n",
    "best_models_list = [best_model_svc, best_model_xgb, best_model_et]\n",
    "\n",
    "for model in best_models_list: \n",
    "    scores = cv_one_class_classification(model, X_EEGxADHD, label_column, n_splits = 5)\n",
    "    \n",
    "#SWAN + EEG + Sex + Age\n",
    "\n",
    "best_model_svc = SVC().set_params(**(bests_SWAN_EEG['SVC']['accuracy']['parameters']))\n",
    "best_model_xgb = XGBClassifier().set_params(**(bests_SWAN_EEG['XGBClassifier']['accuracy']['parameters']))\n",
    "best_model_et = ExtraTreesClassifier().set_params(**(bests_SWAN_EEG['ExtraTreesClassifier']['accuracy']['parameters']))\n",
    "\n",
    "best_models_list = [best_model_svc, best_model_xgb, best_model_et]\n",
    "\n",
    "for model in best_models_list: \n",
    "    scores = cv_one_class_classification(model, ADHD_SWAN, label_column, n_splits = 5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers do not always predict the most frequent class, which means that model is not 'ill-posed'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting diagnosis vs no-diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to predict from EEG data whether a patient is healthy (no-diagnosis given, positive samples) or ill (diagnosis given, negative samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cats = list(set(eeg_clusters['DX_01_Cat'].values))\n",
    "\n",
    "print(cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create healthy/ill (HI) dataset\n",
    "\n",
    "HI = eeg_clusters.copy()\n",
    "\n",
    "# remove incomplete evaluation\n",
    "to_drop_inc_eval = []\n",
    "for sample in HI.index: \n",
    "    if HI.loc[sample,'DX_01_Cat'] == 'No Diagnosis Given: Incomplete Eval': \n",
    "        to_drop_inc_eval.append(sample)\n",
    "        \n",
    "\n",
    "HI = HI.drop(to_drop_inc_eval, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dict for healthy/ill mapping\n",
    "d = { \n",
    "    'No Diagnosis Given' : 1\n",
    "}\n",
    "cats.remove('No Diagnosis Given')\n",
    "for cat in cats: \n",
    "    d[cat]=0\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create label column\n",
    "HI['DX_01_Cat'] = HI['DX_01_Cat'].map(d).fillna(0).astype('int64')\n",
    "\n",
    "# remove othe diagnoses-related columns\n",
    "HI = HI.drop(['DX_01_Sub', 'DX_01'], axis=1)\n",
    "\n",
    "#rename label column\n",
    "HI = HI.rename(columns={'DX_01_Cat': 'label'})\n",
    "\n",
    "HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop age column \n",
    "HI.pop('Age')\n",
    "HI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data HI\n",
    "HI_test = HI.loc[HI['id'].isin(test_ids_l)]\n",
    "HI_train = HI.loc[~HI['id'].isin(test_ids_l)]\n",
    "\n",
    "#pop id and label column train\n",
    "id_column_HI_train = HI_train.pop('id')\n",
    "label_HI_train = HI_train.pop('label')\n",
    "\n",
    "#pop id and label column test \n",
    "id_column_HI_test = HI_test.pop('id')\n",
    "label_HI_test = HI_test.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HI_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE 1\n",
    "\n",
    "imp_strategy = 'median'\n",
    "\n",
    "models = {\n",
    "    'RidgeClassifier' : 10, \n",
    "    'LinearSVC': 20,\n",
    "    'SVC' : 20,\n",
    "    'RandomForestClassifier' : 20, \n",
    "    'ExtraTreesClassifier' : 20, \n",
    "    'AdaBoostClassifier' : 20, \n",
    "    'XGBClassifier' : 20\n",
    "}\n",
    "results_HI_EEG, bests_HI_EEG = one_class_classify_CV(X=HI_train, y=label_HI_train, models=models, CV_n_splits=5, verbose=0, impute_strategy=imp_strategy)\n",
    "\n",
    "# PIPELINE 2 \n",
    "# decide to go for the accuracy metric of the three\n",
    "selected_metric = 'balanced_accuracy'\n",
    "\n",
    "model_classes_dict = {\n",
    "    \n",
    "    'RidgeClassifier' : RidgeClassifier(), \n",
    "    'LinearSVC': LinearSVC(),\n",
    "    'SVC' : SVC(),\n",
    "    'RandomForestClassifier' : RandomForestClassifier(), \n",
    "    'ExtraTreesClassifier' : ExtraTreesClassifier(), \n",
    "    'AdaBoostClassifier' : AdaBoostClassifier(), \n",
    "    'XGBClassifier' : XGBClassifier()\n",
    "}\n",
    "\n",
    "for model_class_name, model_class in model_classes_dict.items():\n",
    "    parameters = bests_HI_EEG[model_class_name][selected_metric]['parameters']\n",
    "    params=make_pipe_model_params(parameters)\n",
    "    model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('model_class', model_class)])\n",
    "    model.set_params(**params)\n",
    "    results = cv_one_class_classification(model, HI_train, label_HI_train, n_splits=5)\n",
    "    b_acc_med = np.median(np.array(results['balanced_accuracy']))\n",
    "    f1_score_med = np.median(np.array(results['f1_score']))\n",
    "    b_acc_m = np.array(results['balanced_accuracy']).mean()\n",
    "    f1_score_m = np.array(results['f1_score']).mean()\n",
    "    print('\\n\\n',\n",
    "          model_class_name, '\\n\\t balanced_accuracy:  mean {} \\t median {}' \n",
    "          '\\n\\t f1_score:  mean {} \\t median {} \\n\\n'.format(b_acc_m, b_acc_med, f1_score_m, f1_score_med))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PIPELINE 3\n",
    "# select best model (manually) and predict on test set \n",
    "\n",
    "best_model_class = 'XGBClassifier'\n",
    "\n",
    "best_model_parameters = bests_HI_EEG[best_model_class][selected_metric]['parameters']\n",
    "best_model_params = make_pipe_model_params(best_model_parameters)\n",
    "best_estimator = model_classes_dict[best_model_class]\n",
    "best_model = Pipeline([('imputing',SimpleImputer(strategy=imp_strategy)),('scaling', StandardScaler()), ('model_class', best_estimator)])\n",
    "best_model.set_params(**best_model_params)\n",
    "\n",
    "best_model.fit(HI_train, label_HI_train)\n",
    "best_model_preds = best_model.predict(HI_test)\n",
    "\n",
    "\n",
    "best_model_test_b_accuracy = balanced_accuracy_score(label_HI_test, best_model_preds)\n",
    "best_model_test_f1_score= f1_score(label_HI_test, best_model_preds)\n",
    "print('\\n\\n Best model results test set: \\n balanced_accuracy {} \\t f1_score {} \\n'.format(best_model_test_b_accuracy, best_model_test_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try with randomized search and multiple metrics instead of my function (NOT WORKING, SLOW)\n",
    "'''\n",
    "def rando_search_model_class (X, y, model_class, model_class_name, param_grid, n_combinations):\n",
    "    \n",
    "    from sklearn.metrics import make_scorer\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    inner_cv = KFold(n_splits=5, shuffle=True)   # no random state passed\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True)   # no random state passed\n",
    "    \n",
    "    scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score), \n",
    "           'recall': make_scorer(recall_score)}\n",
    "    \n",
    "    # accuracy re-fit and nested scoring\n",
    "    rgs_acc = RandomizedSearchCV(estimator=model_class, param_distributions=param_grid, n_iter=n_combinations, scoring=scoring, cv=inner_cv, refit='accuracy')\n",
    "    rgs_acc.fit(X,y)\n",
    "    model_nested_score_acc = cross_val_score(rgs_acc, X=X, y=y, cv=outer_cv)\n",
    "    print('DONE ACCURACY')\n",
    "\n",
    "    # precision re-fit and nested scoring\n",
    "    rgs_prc = RandomizedSearchCV(estimator=model_class, param_distributions=param_grid, n_iter=n_combinations, scoring=scoring, cv=inner_cv, refit='precision')\n",
    "    rgs_prc.fit(X,y)\n",
    "    model_nested_score_prc = cross_val_score(rgs_prc, X=X, y=y, cv=outer_cv)\n",
    "    print('DONE PRECISION')\n",
    "\n",
    "    # recall re-fit and nested scoring\n",
    "    rgs_rec = RandomizedSearchCV(estimator=model_class, param_distributions=param_grid, n_iter=n_combinations, scoring=scoring, cv=inner_cv, refit='recall')\n",
    "    rgs_rec.fit(X,y)\n",
    "    model_nested_score_rec = cross_val_score(rgs_rec, X=X, y=y, cv=outer_cv)\n",
    "    print('DONE RECALL')\n",
    "\n",
    "    best_models = {\n",
    "        'accuracy': rgs_acc, \n",
    "        'precision': rgs_prc, \n",
    "        'recall': rgs_rec\n",
    "    }\n",
    "    best_results = {\n",
    "        'accuracy': [model_nested_score_acc.mean(), model_nested_score_acc.median()],\n",
    "        'precision': [model_nested_score_prc.mean(), model_nested_score_prc.median()], \n",
    "        'recall': [model_nested_score_rec.mean(), model_nested_score_rec.median()]\n",
    "    }\n",
    "\n",
    "    return best_models, best_results\n",
    "    \n",
    "model_class = ExtraTreesClassifier()\n",
    "model_class_name = 'ExtraTreesClassifier'\n",
    "\n",
    "param_grid ={\n",
    "    'n_estimators' : [200, 250, 300],\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'min_samples_split' : [2, 3, 4],\n",
    "    'bootstrap' : [True, False],\n",
    "    'class_weight' : ['balanced', 'balanced_subsample'],\n",
    "    'verbose' : [0],\n",
    "    'n_jobs' : [3]\n",
    "    }\n",
    "\n",
    "best_models,best_results = rando_search_model_class(X_swan_train,label_train, model_class,model_class_name,param_grid, 3 )\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
