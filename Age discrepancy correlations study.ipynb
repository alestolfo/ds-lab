{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try to correlate age discrepancy with some scores of the behavioral dataset. Our model to predict age uses only the structural MRI features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alestolfo/anaconda2/envs/ds-lab/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import create_dataset_mri, cv_for_cde, create_dataset_eeg\n",
    "from cde.density_estimator import MixtureDensityNetwork\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.python.keras.activations import tanh\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix age threshold\n",
    "threshold = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavioral data\n",
    "behavioral = pd.read_csv('data/Behavioral/cleaned/HBNFinalSummaries.csv')\n",
    "# Create dataset MRI\n",
    "target = 'Age'\n",
    "data = create_dataset_mri(SCORE = target)\n",
    "\n",
    "test = data.loc[data['DX_01'].isin(['Autism Spectrum Disorder', 'ADHD-Combined Type', 'ADHD-Inattentive Type', 'Specific Learning Disorder with Impairment in Reading'])]\n",
    "healthy = data.loc[data['DX_01'].isin(['No Diagnosis Given'])]\n",
    "train = data.loc[~data['DX_01'].isin(['Autism Spectrum Disorder', 'ADHD-Combined Type', 'ADHD-Inattentive Type', 'No Diagnosis Given', 'Specific Learning Disorder with Impairment in Reading'])]\n",
    "\n",
    "test = test = test[test['Age']<threshold]\n",
    "healthy = healthy[healthy['Age']<threshold]\n",
    "train = train[train['Age']<threshold]\n",
    "\n",
    "\n",
    "train.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)\n",
    "healthy.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)\n",
    "test.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train = np.array(train)\n",
    "ID_train_init = train[:,0]\n",
    "X_train = train[:,2:]\n",
    "y_train = train[:, 1]\n",
    "y_train = y_train.reshape((-1,1))\n",
    "\n",
    "# test\n",
    "test = np.array(test)\n",
    "ID_test_init = test[:,0]\n",
    "X_test = test[:,2:]\n",
    "y_test = test[:, 1]\n",
    "y_test = y_test.reshape((-1,1))\n",
    "\n",
    "# healthy\n",
    "healthy = np.array(healthy)\n",
    "y_healthy = healthy[:, 1]\n",
    "X_healthy = np.concatenate((np.reshape(healthy[:,0],[-1,1]), healthy[:,2:]), axis = 1)\n",
    "y_healthy = y_healthy.reshape((-1,1))\n",
    "\n",
    "\n",
    "X_test_init = np.array(X_test, dtype=np.float64)\n",
    "y_test_init = np.array(y_test, dtype=np.float64)\n",
    "X_train_init = np.array(X_train, dtype=np.float64)\n",
    "y_train_init  = np.array(y_train, dtype=np.float64)\n",
    "\n",
    "# split the healthy\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_healthy, y_healthy, test_size=0.5, random_state=8)\n",
    "y_train_h = y_train_h.reshape((-1,1))\n",
    "y_test_h = y_test_h.reshape((-1,1))\n",
    "ID_train_h = X_train_h[:,0]\n",
    "X_train_h = X_train_h[:,1:]\n",
    "ID_test_h = X_test_h[:,0]\n",
    "X_test_h = X_test_h[:,1:]\n",
    "y_train_h = np.array(y_train_h, dtype=np.float64)\n",
    "X_train_h = np.array(X_train_h, dtype=np.float64)\n",
    "y_test_h = np.array(y_test_h, dtype=np.float64)\n",
    "X_test_h = np.array(X_test_h, dtype=np.float64)\n",
    "# Now add again\n",
    "ID_test = np.concatenate((ID_test_init, ID_test_h))\n",
    "y_test = np.concatenate((y_test_init, y_test_h))\n",
    "X_test = np.concatenate((X_test_init, X_test_h))\n",
    "\n",
    "ID_train = np.concatenate((ID_train_init, ID_train_h))\n",
    "y_train = np.concatenate((y_train_init, y_train_h))\n",
    "X_train = np.concatenate((X_train_init, X_train_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 18s | loss: 653.036\n",
      "mean log-loss train: 1.4577\n",
      "Test MSE: 3.653655608823112\n"
     ]
    }
   ],
   "source": [
    "# Set model parameters\n",
    "ndim_x=X_train.shape[1]\n",
    "ndim_y=y_train.shape[1]\n",
    "# We try the \"faster decay rate for non-gaussian data\" proposed in the paper: h = n^(-1/(d+1))\n",
    "n = X_train.shape[0]\n",
    "d = X_train.shape[1]+y_train.shape[1]\n",
    "h = n**(-1/(d+1))\n",
    "# Define the model\n",
    "model = MixtureDensityNetwork('MDN', ndim_x, ndim_y, n_centers=10, hidden_sizes=(16, 16), hidden_nonlinearity=tf.nn.tanh,\n",
    "               n_training_epochs=1000, x_noise_std=h, y_noise_std=h, adaptive_noise_fn=None, entropy_reg_coef=0.0,\n",
    "               weight_decay=0.0, weight_normalization=True, data_normalization=True, dropout=0.0, l2_reg=0.0, l1_reg=0.0,\n",
    "               random_seed=42)\n",
    "# Fit\n",
    "model.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = model.mean_(X_test)\n",
    "y_pred = y_pred.reshape((-1,1))\n",
    "y_pred.shape\n",
    "print('Test MSE: {}'.format(mean_squared_error(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define discrepancy\n",
    "std = model.std_(X_test)\n",
    "discrepancy = np.divide((y_test-y_pred), 1+std)\n",
    "#discrepancy = y_test-y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe for test observations with behavioral data + discrepancy\n",
    "data = {'discrepancy':discrepancy[:,0]}\n",
    "discrepancy_df = pd.DataFrame(data)\n",
    "ID_df = pd.DataFrame({'EID':ID_test})\n",
    "discrepancy_merged = pd.concat([ID_df, discrepancy_df], axis=1)\n",
    "dataframe = pd.merge(discrepancy_merged, behavioral, how='inner', on='EID')\n",
    "dataframe = dataframe.drop(['Anonymized.ID', 'Study.Site'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discrepancy              1.000000\n",
       "Age                      0.451420\n",
       "DX_03_New                0.259807\n",
       "DX_04_New                0.340653\n",
       "DX_04_Rem                0.323752\n",
       "CBCL_Pre_WD              0.253926\n",
       "CELF_SA_R                0.277746\n",
       "CLEF5M_FL_Raw            0.273480\n",
       "Picture_Seq_Raw          0.261719\n",
       "PPVT4_RawScore           0.296596\n",
       "sib3dxse                 0.484766\n",
       "TRF_Pre_Attention_Raw    0.359634\n",
       "VL_Comm1_Raw             0.574106\n",
       "VL_Comm_Stnd             0.808613\n",
       "VL_DLS_Scale             0.298061\n",
       "VL_Social_Scale          0.554523\n",
       "WIAT_Num_Raw             0.311534\n",
       "WIAT_Spell_Raw           0.283371\n",
       "WIAT_MP_Raw              0.289897\n",
       "Name: discrepancy, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the correlations\n",
    "correlations = dataframe[dataframe.columns[1:]].corr()['discrepancy'][:]\n",
    "# Inspect correlations\n",
    "correlations[correlations > 0.25]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(566, 446)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['discrepancy',\n",
       " 'Age',\n",
       " 'CTOPP_EL_R',\n",
       " 'CTOPP_BW_R',\n",
       " 'FGC_Curl_Up',\n",
       " 'PCIAT_Total',\n",
       " 'WIAT_Num_Raw',\n",
       " 'WIAT_Spell_Raw',\n",
       " 'WIAT_Word_Raw',\n",
       " 'WIAT_LCRV_Raw',\n",
       " 'WIAT_MP_Raw',\n",
       " 'WISC_BD_Raw']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_corr = []\n",
    "for feat in correlations[correlations > 0.20].index:\n",
    "    if dataframe[feat].isna().sum() < 100:\n",
    "        relevant_corr.append(feat)\n",
    "relevant_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that there is an interesting correlation between Wechsler Individual Achievement Test (WIAT) and the age discrepancy. Let us now study whether this relationship is not due to the correlation between age and age discrepancy. I.e. let us see if the age discrepancy is giving us non-redundant (w.r.t the age) information about WIAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discrepancy       0.451420\n",
       "Age               1.000000\n",
       "CTOPP_EL_R        0.430704\n",
       "CTOPP_BW_R        0.441091\n",
       "FGC_Curl_Up       0.577915\n",
       "PCIAT_Total       0.425690\n",
       "WIAT_Num_Raw      0.778509\n",
       "WIAT_Spell_Raw    0.713854\n",
       "WIAT_Word_Raw     0.639737\n",
       "WIAT_LCRV_Raw     0.618837\n",
       "WIAT_MP_Raw       0.731364\n",
       "WISC_BD_Raw       0.530942\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_correlations = dataframe[dataframe.columns[1:]].corr()['Age'][:]\n",
    "age_correlations[relevant_corr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is high correlation between age and WIAT, but for CTOPP and PCIAT it is lower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    sum_sq     df           F        PR(>F)\n",
      "discrepancy     597.893626    1.0    2.874048  9.061228e-02\n",
      "Age           68750.404531    1.0  330.480090  1.276242e-57\n",
      "Residual     109008.721299  524.0         NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "model = ols('WIAT_Word_Raw ~ discrepancy + Age', data = dataframe).fit()\n",
    "                \n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "print (anova_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discrepancy': 0.0,\n",
       " 'Age': 1.0,\n",
       " 'CTOPP_EL_R': 0.7605612970528647,\n",
       " 'CTOPP_BW_R': 0.8978141472047116,\n",
       " 'FGC_Curl_Up': 0.4141129979267989,\n",
       " 'PCIAT_Total': 0.3882321985743732,\n",
       " 'WIAT_Num_Raw': 0.09744599509459974,\n",
       " 'WIAT_Spell_Raw': 0.15299767574839895,\n",
       " 'WIAT_Word_Raw': 0.09061227711565088,\n",
       " 'WIAT_LCRV_Raw': 0.05194419477173275,\n",
       " 'WIAT_MP_Raw': 0.06642134014895226,\n",
       " 'WISC_BD_Raw': 0.861677651437721}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvals = dict()\n",
    "\n",
    "for feat in relevant_corr:\n",
    "    model = ols('{} ~ discrepancy + Age'.format(feat), data = dataframe).fit()\n",
    "    anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "    \n",
    "    pvals[feat] = anova_result['PR(>F)']['discrepancy']\n",
    "                \n",
    "pvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider the most frequent situations: ADHD Inattentive, ADHD Combined, Autism, Healthy. Specific impairment in Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe.loc[dataframe['DX_01'].isin(['ADHD-Combined Type', 'ADHD-Inattentive Type','No Diagnosis Given', 'Autism Spectrum Disorder', 'Specific Learning Disorder with Impairment in Reading'])]\n",
    "df = df[df['Age']<threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate these quantities as an average over different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def discrepancy_corr(repetitions, multiple_testing_correction = 'fdr_bh', exclude_old = True, threshold = threshold, corr_threshold = 0.2):\n",
    "    # Behavioral data\n",
    "    behavioral = pd.read_csv('data/Behavioral/cleaned/HBNFinalSummaries.csv')\n",
    "    # Create dataset MRI\n",
    "    target = 'Age'\n",
    "    data = create_dataset_mri(SCORE = target)\n",
    "\n",
    "    test = data.loc[data['DX_01'].isin(['Autism Spectrum Disorder', 'ADHD-Combined Type', 'ADHD-Inattentive Type', 'Specific Learning Disorder with Impairment in Reading'])]\n",
    "    healthy = data.loc[data['DX_01'].isin(['No Diagnosis Given'])]\n",
    "    train = data.loc[~data['DX_01'].isin(['Autism Spectrum Disorder', 'ADHD-Combined Type', 'ADHD-Inattentive Type', 'No Diagnosis Given', 'Specific Learning Disorder with Impairment in Reading'])]\n",
    "    \n",
    "    if exclude_old == True:\n",
    "        # Remove patients aged > threshold\n",
    "        test = test = test[test['Age']<threshold]\n",
    "        healthy = healthy[healthy['Age']<threshold]\n",
    "        train = train[train['Age']<threshold]\n",
    "    \n",
    "    \n",
    "    train.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)\n",
    "    healthy.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)\n",
    "    test.drop(columns=['DX_01_Cat', 'DX_01_Sub', 'DX_01'], inplace=True)\n",
    "\n",
    "    \n",
    "    # train\n",
    "    train = np.array(train)\n",
    "    ID_train_init = train[:,0]\n",
    "    X_train = train[:,2:]\n",
    "    y_train = train[:, 1]\n",
    "    y_train = y_train.reshape((-1,1))\n",
    "\n",
    "    # test\n",
    "    test = np.array(test)\n",
    "    ID_test_init = test[:,0]\n",
    "    X_test = test[:,2:]\n",
    "    y_test = test[:, 1]\n",
    "    y_test = y_test.reshape((-1,1))\n",
    "\n",
    "    # healthy\n",
    "    healthy = np.array(healthy)\n",
    "    y_healthy = healthy[:, 1]\n",
    "    X_healthy = np.concatenate((np.reshape(healthy[:,0],[-1,1]), healthy[:,2:]), axis = 1)\n",
    "    y_healthy = y_healthy.reshape((-1,1))\n",
    "\n",
    "    \n",
    "    X_test_init = np.array(X_test, dtype=np.float64)\n",
    "    y_test_init = np.array(y_test, dtype=np.float64)\n",
    "    X_train_init = np.array(X_train, dtype=np.float64)\n",
    "    y_train_init  = np.array(y_train, dtype=np.float64)\n",
    "    \n",
    "    # things I want to compute\n",
    "    t_adhd = []\n",
    "    t_adhd_combined = []\n",
    "    t_autism = [] \n",
    "    t_impaired = []\n",
    "    \n",
    "    pvals = dict()\n",
    "    \n",
    "    for i in range(repetitions):\n",
    "        # split the healthy\n",
    "        X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_healthy, y_healthy, test_size=0.5, random_state=i)\n",
    "        y_train_h = y_train_h.reshape((-1,1))\n",
    "        y_test_h = y_test_h.reshape((-1,1))\n",
    "        ID_train_h = X_train_h[:,0]\n",
    "        X_train_h = X_train_h[:,1:]\n",
    "        ID_test_h = X_test_h[:,0]\n",
    "        X_test_h = X_test_h[:,1:]\n",
    "        y_train_h = np.array(y_train_h, dtype=np.float64)\n",
    "        X_train_h = np.array(X_train_h, dtype=np.float64)\n",
    "        y_test_h = np.array(y_test_h, dtype=np.float64)\n",
    "        X_test_h = np.array(X_test_h, dtype=np.float64)\n",
    "        # Now add again\n",
    "        ID_test = np.concatenate((ID_test_init, ID_test_h))\n",
    "        y_test = np.concatenate((y_test_init, y_test_h))\n",
    "        X_test = np.concatenate((X_test_init, X_test_h))\n",
    "\n",
    "        ID_train = np.concatenate((ID_train_init, ID_train_h))\n",
    "        y_train = np.concatenate((y_train_init, y_train_h))\n",
    "        X_train = np.concatenate((X_train_init, X_train_h))\n",
    "    \n",
    "        # Set model parameters\n",
    "        ndim_x=X_train.shape[1]\n",
    "        ndim_y=y_train.shape[1]\n",
    "        # We try the \"faster decay rate for non-gaussian data\" proposed in the paper: h = n^(-1/(d+1))\n",
    "        n = X_train.shape[0]\n",
    "        d = X_train.shape[1]+y_train.shape[1]\n",
    "        h = n**(-1/(d+1))\n",
    "        model = MixtureDensityNetwork('h-{}'.format(i), ndim_x, ndim_y, n_centers=10, hidden_sizes=(16, 16), hidden_nonlinearity=tf.nn.tanh,\n",
    "               n_training_epochs=1000, x_noise_std=h, y_noise_std=h, adaptive_noise_fn=None, entropy_reg_coef=0.0,\n",
    "               weight_decay=0.0, weight_normalization=True, data_normalization=True, dropout=0.0, l2_reg=0.0, l1_reg=0.0,\n",
    "               random_seed=42)\n",
    "        \n",
    "        # Fit\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict\n",
    "        y_pred = model.mean_(X_test)\n",
    "        y_pred = y_pred.reshape((-1,1))\n",
    "        # Define discrepancy\n",
    "        std = model.std_(X_test)\n",
    "        discrepancy = np.divide((y_test-y_pred), 1+std)\n",
    "        # Get dataframe for test observations with behavioral data + discrepancy\n",
    "        data = {'discrepancy':discrepancy[:,0]}\n",
    "        discrepancy_df = pd.DataFrame(data)\n",
    "        ID_df = pd.DataFrame({'EID':ID_test})\n",
    "        discrepancy_merged = pd.concat([ID_df, discrepancy_df], axis=1)\n",
    "        df = pd.merge(discrepancy_merged, behavioral, how='inner', on='EID')\n",
    "        \n",
    "        print(df.columns)\n",
    "        \n",
    "        # Get the correlations\n",
    "        correlations = df[df.columns[1:]].corr()['discrepancy'][:]      \n",
    "        \n",
    "        for feat in correlations[correlations > corr_threshold].index:\n",
    "            if df[feat].isna().sum() < 100:\n",
    "                print(feat)\n",
    "                model = ols('{} ~ discrepancy + Age'.format(feat), data = df).fit()\n",
    "                anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "    \n",
    "                if pvals.get(feat, -1) == -1:\n",
    "                    pvals[feat] = [anova_result['PR(>F)']['discrepancy']]\n",
    "                else:\n",
    "                    pvals[feat].append(anova_result['PR(>F)']['discrepancy'])\n",
    "                    \n",
    "    return pvals\n",
    "        \n",
    "    #print('Mean t-test p-val (inattentive vs healthy): {} \\n Mean t-test p-val (combined vs healthy): {} \\n Mean t-test p-val (autism vs healthy): {}, \\n Mean t-test p-val (impaired vs healthy): {}'.format(np.mean(t_adhd), np.mean(t_adhd_combined), np.mean(t_autism), np.mean(t_impaired)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 56s | loss: 645.285\n",
      "mean log-loss train: 1.4404\n",
      "discrepancy\n",
      "Age\n",
      "CTOPP_EL_R\n",
      "CTOPP_BW_R\n",
      "FGC_Curl_Up\n",
      "PCIAT_Total\n",
      "WIAT_Num_Raw\n",
      "WIAT_Spell_Raw\n",
      "WIAT_Word_Raw\n",
      "WIAT_LCRV_Raw\n",
      "WIAT_MP_Raw\n",
      "WISC_BD_Raw\n",
      "WISC_Similarities_Raw\n",
      "WISC_MR_Raw\n",
      "WISC_Vocab_Raw\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 59s | loss: 633.623\n",
      "mean log-loss train: 1.4143\n",
      "discrepancy\n",
      "Age\n",
      "CTOPP_EL_R\n",
      "CTOPP_BW_R\n",
      "FGC_Curl_Up\n",
      "PCIAT_Total\n",
      "WIAT_Num_Raw\n",
      "WIAT_Spell_Raw\n",
      "WIAT_LCRV_Raw\n",
      "WIAT_MP_Raw\n",
      "WISC_BD_Raw\n",
      "WISC_Similarities_Raw\n",
      "WISC_Vocab_Raw\n",
      "WISC_VP_Raw\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 60s | loss: 665.333\n",
      "mean log-loss train: 1.4851\n",
      "discrepancy\n",
      "Age\n",
      "CTOPP_EL_R\n",
      "CTOPP_BW_R\n",
      "FGC_Curl_Up\n",
      "WIAT_Num_Raw\n",
      "WIAT_Pseudo_Raw\n",
      "WIAT_Spell_Raw\n",
      "WIAT_Word_Raw\n",
      "WIAT_LCRV_Raw\n",
      "WIAT_MP_Raw\n",
      "WISC_BD_Raw\n",
      "WISC_Similarities_Raw\n",
      "WISC_MR_Raw\n",
      "WISC_Vocab_Raw\n",
      "WISC_VP_Raw\n"
     ]
    }
   ],
   "source": [
    "res = discrepancy_corr(5, corr_threshold = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Benjamini-Hochberg correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>df</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>discrepancy</th>\n",
       "      <td>0.408809</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016510</td>\n",
       "      <td>8.978141e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>2169.274895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.608667</td>\n",
       "      <td>3.229591e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>11711.935090</td>\n",
       "      <td>473.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sum_sq     df          F        PR(>F)\n",
       "discrepancy      0.408809    1.0   0.016510  8.978141e-01\n",
       "Age           2169.274895    1.0  87.608667  3.229591e-19\n",
       "Residual     11711.935090  473.0        NaN           NaN"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ols('CTOPP_BW_R ~ discrepancy + Age', data = df).fit()\n",
    "\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'discrepancy': [0.0],\n",
       " 'Age': [1.0],\n",
       " 'CTOPP_EL_R': [0.7605612970528647],\n",
       " 'CTOPP_BW_R': [0.8978141472047116],\n",
       " 'FGC_Curl_Up': [0.4141129979267989],\n",
       " 'PCIAT_Total': [0.3882321985743732],\n",
       " 'WIAT_Num_Raw': [0.09744599509459974],\n",
       " 'WIAT_Spell_Raw': [0.15299767574839895],\n",
       " 'WIAT_Word_Raw': [0.09061227711565088],\n",
       " 'WIAT_LCRV_Raw': [0.05194419477173275],\n",
       " 'WIAT_MP_Raw': [0.06642134014895226],\n",
       " 'WISC_BD_Raw': [0.861677651437721]}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (ds-lab)",
   "language": "python",
   "name": "ds-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
